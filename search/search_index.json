{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Learning Group","text":"<p>Welcome to the official repository for the Toronto AI Learning Group - a collaborative space where community-led sessions, resources, and discussions are organized and shared.</p>"},{"location":"#in-one-line-from-chatgpt","title":"\ud83c\udfaf In one line from ChatGPT","text":"<p>The culture of open sharing, learning, and collaboration is the engine of progress in AI.  Frameworks like TensorFlow, PyTorch, Hugging Face Transformers, scikit-learn and even Linux/Ubuntu that powered most AI systems itself are open-source. Without them, no AI researcher or startup could move quickly.</p> <p>Real growth comes from sharing inspiring learning content, connecting ideas, building practical use cases, and diving deep with curiosity and creativity. Its always to good have someone to talk to, feels like home. Isn't Learning in a group super fun..? </p> <p>Sharing promotions, referrals, or marketing content in a group is fine. Please note that they mostly drive usage and sales and they rarely contribute to true learning or collaborative building.   </p> <p>Proprietary tools are the vehicles that make that progress practical, accessible, and sustainable. But without the engine, the vehicle goes nowhere. </p> <p>Objective &amp; Focus of the Group is Collaborative Learning and Collective Growth to foster Collective Intelligence. </p> <ul> <li>\u201cNone of us is as smart as all of us.\u201d \u2014 Ken Blanchard</li> <li>\u201cIf everyone is moving forward together, then success takes care of itself.\u201d \u2014 Henry Ford</li> <li>\u201cThe whole is greater than the sum of its parts.\u201d \u2014 Aristotle</li> <li> <p>\u201cAlone we can do so little; together we can do so much.\u201d \u2014 Helen Keller</p> </li> <li> <p>\u201cWe just need to lay a brick each day, without fail, the wall will be built.\u201d</p> </li> </ul>"},{"location":"#community-ethos","title":"\ud83e\udd1d Community Ethos","text":"<p>Community Value &amp; Learning </p> <p>Group Repository reflects the collaborative spirit of our learning group. We encourage respectful dialogue, curiosity-driven exploration, and inclusive participation.</p>"},{"location":"#purpose","title":"\ud83d\udcda Purpose","text":"<p>This repository serves as a centralized archive for all materials presented or referenced during our learning sessions. By consolidating content in one place, we aim to foster accessibility, continuity, and shared growth across our community of learners and educators.</p>"},{"location":"#scheduling-sessions","title":"\ud83d\uddd3\ufe0f Scheduling Sessions","text":"<p>Groups Open Sessoin Calendar</p> <p>Groups Google Calendar for Reminders</p> <p>Sessions are added on a rolling basis by contributors and facilitators. If you would like to contribute materials or suggest improvements to the structure, feel free to open an issue or submit a pull request.</p>"},{"location":"#happy-learning","title":"Happy Learning! \ud83d\ude0a","text":""},{"location":"ai-learning-plans/An-approach-to-learn-AI-in-2025-fueled-by-real-world-problem-solving-and-vigor/","title":"2025 - Easy ways to Laern AI","text":""},{"location":"ai-learning-plans/An-approach-to-learn-AI-in-2025-fueled-by-real-world-problem-solving-and-vigor/#objective","title":"Objective","text":"<p>To provide a clear roadmap for beginners to start their AI journey\u2014whether through no-code exploration or coding-based learning\u2014by focusing on practical skills, daily automation, and progressive workflow building.</p>"},{"location":"ai-learning-plans/An-approach-to-learn-AI-in-2025-fueled-by-real-world-problem-solving-and-vigor/#goal","title":"Goal","text":"<ul> <li>Enable beginners to leverage AI effectively in daily life and work.</li> <li>Build confidence in prompting, automation, and workflow design before diving into programming.</li> <li>Offer an alternative technical path (Python + data science libraries) for those aiming at developer or advanced technical roles.</li> <li>Encourage project-based learning and portfolio creation to demonstrate practical expertise.</li> </ul> <ul> <li> <p>Start with No-Code Tools &amp; Accessibility</p> </li> <li> <p>Modern AI tools (e.g., ChatGPT) are built for non-technical users\u2014no coding required.</p> </li> <li>Begin as an \u201cEveryday Explorer\u201d: simplify life, summarize documents, and organize tasks with AI.</li> <li> <p>Even if aiming for technical roles, start with Generative AI (GenAI) first.</p> </li> <li> <p>Prompting is the Core Skill</p> </li> <li> <p>The most essential ability: communicating effectively with AI.</p> </li> <li> <p>Use the simple structure:</p> <ul> <li>Aim \u2192 What you want AI to do</li> <li>Context \u2192 Background/relevant information</li> <li>Rules \u2192 Constraints, format, or style requirements</li> </ul> </li> <li> <p>Practice Through Daily Automation</p> </li> <li> <p>Identify pain points in work/life and apply AI to solve them.</p> </li> <li>Start by automating personal tasks to boost efficiency.</li> <li> <p>Use existing tools (AI Consumers / Level 3), e.g., custom GPTs or content generators.</p> </li> <li> <p>Advance to Workflow Building (No-Code Platforms)</p> </li> <li> <p>Learn \u201cWorkflow Thinking\u201d: break large tasks into smaller steps.</p> </li> <li> <p>Automate and connect steps (\u201cCreative Remixing\u201d) with no-code platforms:</p> <ul> <li>Zapier, Make, N8N, etc.</li> <li>Transition into a Builder (Level 2) \u2192 creating custom tools or agents that can reason and make decisions.</li> </ul> </li> <li> <p>Programming Alternative (If Coding Route Chosen)</p> </li> <li> <p>Set up a stable dev environment (e.g., VS Code).</p> </li> <li> <p>Learn Python fundamentals, then progress to libraries:</p> <ul> <li>NumPy, Pandas, Matplotlib.</li> <li>Focus on project-based learning \u2192 build portfolio projects early.</li> </ul> </li> </ul>"},{"location":"ai-learning-plans/ai-learning-plan-draft/","title":"2025 - AI Learning Path Draft","text":"<p>This roadmap is designed for an absolute beginner with no prior Python knowledge, focusing on the \"Modern Route\" or \"Everyday Explorer/Power User\" approach described in the sources. This strategy emphasizes rapid, impactful learning using accessible tools and prompt engineering, allowing for real-life implementations within a 2-3 month timeframe, without requiring deep theoretical math or coding initially. User are adviced to spend additional time on areas that interests them or skip a step that doesnt interest them. </p> <p>Interactive, Q&amp;A based collaborative learning can help well with rounded progress and widen activities and spin up interesting disucssions for brainstorming and fun all at the same time.</p> <p>If the goal is rapid progress and achieving practical results in 2-3 months, experts suggest starting with Generative AI applications first, rather than the time-consuming traditional route (Math, Stats, foundational ML).</p>"},{"location":"ai-learning-plans/ai-learning-plan-draft/#ai-roadmap-for-absolute-beginners-23-months","title":"AI Roadmap for Absolute Beginners (2\u20133 Months)","text":"<p>The roadmap is divided into three phases, designed to move the learner from being an AI Consumer (Level 3) to an AI Power User/Tool Builder (approaching Level 2), drawing heavily on no-code/low-code tools and fundamental concepts like prompt engineering.</p>"},{"location":"ai-learning-plans/ai-learning-plan-draft/#phase-1-ai-consumer-and-prompt-mastery-weeks-1-4","title":"Phase 1: AI Consumer and Prompt Mastery (Weeks 1-4)","text":"Duration Core Concept &amp; Goal Key Skills &amp; Actionable Steps Implementation/Taste Week 1 Setting the Stage &amp; Identifying Pain Points Initial Survey/Self-Assessment: Identify current knowledge of no-code tools (e.g., ChatGPT, MidJourney, Claude) and document personal \"pain points\" (tasks causing stress or procrastination). Understand that AI is a large umbrella term, and the immediate focus is on Generative AI (GenAI). Start using a foundational LLM (e.g., ChatGPT or Gemini) to explore daily use cases like summarizing content or drafting emails for efficiency. Week 2 Prompting Fundamentals: Aim, Context, Rules Transition from \"playful prompting\" to structured prompting. Learn the core structure of effective prompting: Aim (what the AI should do), Context (background/audience, often using examples), and Rules (limits, formatting, style). Understand key terms like token (for measuring input/output) and hallucination (AI making things up, which requires double-checking). Practice applying the Aim/Context/Rules structure to 5-10 pain points identified in Week 1 to improve output quality dramatically. Week 3 Role Prompting &amp; Research Tools Learn Role Prompting (assigning the AI a persona, e.g., \"You are a business consultant\") to instantly shape the tone and perspective of the response. Explore specialized research tools (e.g., Perplexity or Notebook LM) that use Retrieval Augmented Generation (RAG) to ground answers in real sources, rather than relying only on training data. Use RAG-enabled tools to synthesize information from a large PDF or article, ensuring the answer is grounded in the provided document (practical application of RAG). Week 4 Workflow Thinking &amp; Tool Stacking Master Workflow Thinking\u2014the ability to break a large, complex task into smaller, manageable steps that AI can handle individually. Understand that stacking three to five solid tools is often more powerful than chasing every new release. Combine two different tools (e.g., an LLM for scripting and an image generator for thumbnails) to solve a small, multi-step creative problem (Creative Remixing)."},{"location":"ai-learning-plans/ai-learning-plan-draft/#phase-2-ai-power-user-and-tool-stacking-weeks-5-8","title":"Phase 2: AI Power User and Tool Stacking (Weeks 5-8)","text":"<p>This phase focuses on leveraging specialized tools and combining them, which is where high-impact results often emerge.</p> Duration Core Concept &amp; Goal Key Skills &amp; Actionable Steps Implementation/Taste Week 5 Creative Remixing: Image Generation Explore the Image Category (MidJourney, ChatGPT's generator, Ideogram). Learn about the foundation: Diffusion models (starting with noise and removing it based on the prompt). Understand that many specialized tools online are often just \"specialized wrappers\" built on foundation models. Generate branded graphics, illustrations, or website mock-ups using text-to-image tools. Week 6 Creative Remixing: Audio and Video Explore the Audio Category (e.g., Eleven Labs for Text-to-Speech, Suno/Yo for music generation) and the rapidly moving Video Category (Text-to-Video and Image-to-Video). Produce a short piece of content (e.g., a short video with an AI-generated script, AI-generated voiceover, and AI-generated background music) to see the efficiency of tool stacking. Weeks 7-8 No-Code Automation &amp; Workflow Implementation Learn to automate repetitive tasks. Automations follow a fixed, step-by-step sequence. Use platforms like Naden (popular for workflow templates) or Zapier/Make to connect tools. Understand how to break down a larger goal into sequential steps that can be automated. Achieve a Real-Life Implementation: Set up a simple no-code workflow (e.g., automatically posting content from one platform to another, or summarizing daily calendar events and emailing them)."},{"location":"ai-learning-plans/ai-learning-plan-draft/#phase-3-building-a-foundation-scaling-up-weeks-9-12","title":"Phase 3: Building a Foundation &amp; Scaling Up (Weeks 9-12)","text":"<p>This final phase focuses on introducing more advanced concepts like agents and addressing the barrier of coding/Python, allowing the learner to decide if they want to pursue the full AI Engineer path.</p> Duration Core Concept &amp; Goal Key Skills &amp; Actionable Steps Implementation/Taste Week 9 Agentic AI Fundamentals Understand the difference between fixed Automations and dynamic Agents (which can reason, make decisions, and choose actions). Learn the three necessary components of an agent: a Brain (LLM), Memory (to retain context), and Tools (actions it can take). Experiment with simple agent nodes in automation tools (like Naden) to build a basic AI personal assistant prototype (e.g., reading a calendar and summarizing priorities). Week 10 The Necessity of Python &amp; Data Science Skills Recognize that if the goal shifts from using existing tools to building custom applications or scaling beyond subscription limits, coding becomes necessary. Python is the go-to language for AI and data science. Even if not coding yet, understand the basic necessity of Data Science skills (cleaning, sourcing, pre-processing data) because \"garbage in, garbage out\" applies even to cutting-edge models. If the learner chooses to begin coding, start with Python fundamentals, focusing on data manipulation libraries like NumPy and Pandas, as all AI is created from data. (If they choose the Traditional Route, they would need Math and Statistics next). Week 11 Project Portfolio &amp; Specialization Use the projects and implementations completed in Phases 1 and 2 to build a portfolio. This is the point to pick a specialization (e.g., large language models, computer vision, etc.). Focus on reverse engineering existing projects to learn the structure of code and application building. Work on a project from a resource like Kaggle (for data science) or explore available LangChain experiments (for LLMs) to understand structure, even if just following along. Week 12 Monetization &amp; Continuous Learning Step 7 is to monetize these skills (via a job, freelancing, or building a product). The real learning happens under the pressure of a deadline or client request. Establish a routine for continuous learning and upskilling, perhaps by joining a like-minded community. Based on the 3-month experience, reflect on whether the theoretical foundation (Math, Statistics, traditional ML) is now required to fill technical gaps, or if focusing purely on Generative AI building remains the path."},{"location":"ai-learning-plans/ai-learning-plan-draft/#summary-of-recommended-approach","title":"Summary of Recommended Approach","text":"<p>This fast-track plan aligns with the Modern Route which suggests starting with Generative AI first, mastering application building, and then adding Data Science fundamentals later, as opposed to the Traditional Route which requires mastering Math, Stats, and Classical ML before moving to GenAI.</p> Route Recommended for Beginners (Rapid Progress) Timeframe (Approx.) Modern Route (Recommended) Learn Generative AI/LLMs first (2 months), then gradually add Data Science fundamentals. 2-3 months minimum to get practical experience. Traditional Route Recommended for freshers or those who want a strong foundational base. 8 months total (4 months for DS/ML/CV/NLP + 2 months for GenAI + 2 months for Agentic AI). <p>step-by-step roadmap for AI beginners, balancing open resources (learning, building foundations) with proprietary tools (practical exposure, scale).</p>"},{"location":"ai-learning-plans/ai-learning-plan-draft/#ai-beginner-roadmap-open-first-proprietary-next","title":"\ud83c\udf31 AI Beginner Roadmap: Open First, Proprietary Next","text":""},{"location":"ai-learning-plans/ai-learning-plan-draft/#stage-1-foundations-open-resources-100","title":"Stage 1: Foundations (Open Resources \u2014 100%)","text":"<ul> <li>\ud83d\udd39 Learn Python basics (lists, loops, dicts, functions).</li> <li>\ud83d\udd39 Get familiar with NumPy, Pandas, Matplotlib \u2192 data handling.</li> <li>\ud83d\udd39 Study basic ML (linear regression, classification) with scikit-learn.</li> <li>\ud83d\udd39 Use open datasets (Iris, MNIST, Titanic Kaggle challenge).</li> <li>\ud83c\udfaf Goal: Understand how AI actually learns from data.</li> </ul>"},{"location":"ai-learning-plans/ai-learning-plan-draft/#stage-2-first-projects-open-80-proprietary-20","title":"Stage 2: First Projects (Open = 80%, Proprietary = 20%)","text":"<ul> <li>\ud83d\udd39 Build simple models \u2192 spam filter, image classifier, chatbot.</li> <li>\ud83d\udd39 Explore Hugging Face small models (BERT mini, GPT-2).</li> <li>\ud83d\udd39 Start Kaggle competitions \u2192 practice + community.</li> <li>\ud83d\udd39 Use ChatGPT/Claude for debugging help or brainstorming, not for the whole project.</li> <li>\ud83c\udfaf Goal: Move from learner \u2192 maker. Gain confidence building end-to-end projects.</li> </ul>"},{"location":"ai-learning-plans/ai-learning-plan-draft/#stage-3-practical-ai-open-60-proprietary-40","title":"Stage 3: Practical AI (Open = 60%, Proprietary = 40%)","text":"<ul> <li>\ud83d\udd39 Learn about fine-tuning (LoRA, transfer learning).</li> <li>\ud83d\udd39 Explore LangChain, Haystack for building AI apps.</li> <li>\ud83d\udd39 Use proprietary APIs (OpenAI, Anthropic, Gemini) to see how industry solutions are deployed.</li> <li>\ud83c\udfaf Goal: Understand both DIY and plug-and-play approaches.</li> </ul>"},{"location":"ai-learning-plans/ai-learning-plan-draft/#stage-4-advanced-applied-open-50-proprietary-50","title":"Stage 4: Advanced &amp; Applied (Open = 50%, Proprietary = 50%)","text":"<ul> <li>\ud83d\udd39 Contribute to open-source repos (fix bugs, write docs, small PRs).</li> <li>\ud83d\udd39 Try deploying your models on cloud (Hugging Face Spaces, Streamlit, Gradio).</li> <li>\ud83d\udd39 Study MLOps basics \u2192 versioning, pipelines, monitoring.</li> <li>\ud83d\udd39 Compare your work to proprietary tools \u2192 learn what\u2019s optimized.</li> <li>\ud83c\udfaf Goal: Become a well-rounded AI builder who can switch between open and closed ecosystems.</li> </ul> <p>\u2705 Final Balance for Beginners:</p> <ul> <li>Learn with open tools \u2192 build core skills.</li> <li>Use proprietary tools \u2192 understand scale, UX, and industry needs.</li> <li>Don\u2019t just consume; always create at every stage.</li> </ul>"},{"location":"ai-pc/hardware-for-running-heavy-llms/","title":"CPU GPU for heavy LLMs","text":""},{"location":"ai-pc/hardware-for-running-heavy-llms/#hardware-configuration-for-medium-to-heavy-llms","title":"Hardware Configuration for Medium to Heavy LLMs","text":"Component For 20B Parameter Models For 80B Parameter Models GPU (NVIDIA) Higher Cost: NVIDIA RTX 4090 (24 GB VRAM)Budget: NVIDIA RTX 3090 (24 GB VRAM) on the used market, or NVIDIA RTX 4080 (16 GB VRAM) for a new card. Higher Cost: NVIDIA RTX 5090 (32 GB VRAM)Budget: Dual NVIDIA RTX 3090s (24 GB VRAM each) or a professional card like the NVIDIA RTX A6000 (48 GB VRAM) if available. GPU (AMD) Best Value: AMD Radeon RX 7900 XTX (24 GB VRAM)Budget: AMD Radeon RX 7900 XT (20 GB VRAM) Higher Cost: AMD Instinct MI300X (192 GB VRAM)Budget: Dual AMD Radeon RX 7900 XTX (24 GB VRAM each) GPU Interconnect N/A High-speed links like NVLink (for NVIDIA) or a motherboard with PCIe 4.0/5.0 for efficient communication between GPUs. CPU (Central Processing Unit) Higher Cost: AMD Ryzen 9 9950X3D or Intel Core i9-14900K.Budget: AMD Ryzen 7 7800X3D or Intel Core i7-14700K. Higher Cost: AMD Ryzen 9 9950X3D or Intel Core i9-14900K. The new AMD Ryzen AI 9 395+ processors also offer excellent performance-per-watt for AI tasks. System RAM Higher Cost: 64 GB+ of DDR5 RAM (e.g., 6000MHz+)Budget: 32 GB of DDR5 RAM (e.g., 5600MHz) Higher Cost: 128 GB+ of DDR5 ECC RAMBudget: 64 GB of DDR5 RAM (e.g., 5600MHz) Storage (NVMe SSD) Higher Cost: A PCIe 5.0 NVMe SSD (e.g., Crucial T705) with sequential read/write speeds of up to 14,000 MB/s.Budget: A high-end PCIe 4.0 NVMe SSD (e.g., Samsung 990 Pro or WD Black SN850X) with speeds of \\~7,000 MB/s. Higher Cost: Multiple high-capacity PCIe 5.0 NVMe SSDs in a RAID configuration for maximum throughput.Budget: A high-capacity PCIe 4.0 NVMe SSD (e.g., Samsung 990 Pro 4TB) with a capacity of 4 TB+. Power Supply A high-wattage PSU (850W+) is recommended. A high-wattage PSU (1000W+) is essential for multi-GPU setups. Cooling Robust cooling for the GPU and CPU is important to prevent performance throttling. Robust cooling for the GPU and CPU is essential to handle the heat from a multi-GPU setup. Interconnect Description Primary Use Case in AI/LLMs Key Advantage Supported GPUs NVLink A high-bandwidth, low-latency GPU-to-GPU interconnect developed by NVIDIA. It allows GPUs to directly share memory and data. Training large models (e.g., LLMs) on multiple GPUs and high-performance computing. Extremely fast direct GPU-to-GPU communication for memory pooling and data sharing. NVIDIA GPUs (specifically data center and high-end workstation models like A100, H100, and some RTX cards) PCIe (PCI Express) A high-speed serial computer expansion bus standard. It is the primary way GPUs connect to a motherboard. Connecting a single GPU to a CPU for both training and inference. Used for multi-GPU setups where the model is too large for one GPU but communication overhead is not a critical bottleneck. Ubiquitous, industry standard, and widely compatible with all consumer and professional GPUs. Both NVIDIA and AMD GPUs Infinity Fabric A cache-coherent interconnect protocol developed by AMD. It provides high-speed communication between different components, including multiple GPUs and CPU dies. Multi-GPU setups for AI workloads on AMD hardware. It allows for efficient data and memory sharing between compatible GPUs. High-speed, low-latency communication specifically designed for AMD's hardware ecosystem. AMD GPUs (Radeon Instinct and some Radeon Pro series) OCuLink An external physical connector for a PCIe channel. It enables a high-speed, direct PCIe connection to an external device, such as an eGPU enclosure. Connecting external GPUs to devices that are not standard desktop PCs (e.g., mini PCs, laptops) for high-performance AI inference or gaming. Provides a pure, low-latency PCIe connection externally, often outperforming alternatives like Thunderbolt for raw bandwidth. Both NVIDIA and AMD GPUs via eGPU docks"},{"location":"ai-pc/pc-for-running-20b-llm/","title":"\ud83d\ude80 Benefits of Running GPT-OSS:20B Locally","text":""},{"location":"ai-pc/pc-for-running-20b-llm/#1-full-data-privacy","title":"\ud83d\udd12 1. Full Data Privacy","text":"<ul> <li>Everything stays on your machine. No queries, prompts, or datasets ever leave your PC.</li> <li>Ideal if you\u2019re working with sensitive business docs, medical/legal notes, financial records, or even personal journals.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#2-latency-responsiveness","title":"\u26a1 2. Latency &amp; Responsiveness","text":"<ul> <li>No internet round-trip.</li> <li>Local inference = instant responses (tokens stream as soon as GPU/CPU generates them).</li> <li>Perfect for offline use, travel, or areas with poor connectivity.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#3-zero-ongoing-api-costs","title":"\ud83d\udcb8 3. Zero Ongoing API Costs","text":"<ul> <li>Once you\u2019ve bought the hardware, inference is free.</li> <li>Running GPT-OSS:20B locally avoids per-token API charges from cloud providers.</li> <li>Useful if you need large batch processing (e.g., analyzing 10,000 PDFs, summarizing datasets).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#4-full-control-customization","title":"\ud83d\udee0\ufe0f 4. Full Control &amp; Customization","text":"<ul> <li>You can fine-tune or LoRA-train models on your own datasets.</li> <li>Ability to swap between different quantizations (4-bit, 8-bit, FP16) to optimize speed vs quality.</li> <li>Run specialized forks of GPT-OSS:20B tuned for coding, reasoning, or dialogue.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#5-bigger-model-better-reasoning","title":"\ud83e\udde0 5. Bigger Model = Better Reasoning","text":"<ul> <li> <p>GPT-OSS:20B (20 billion parameters) is much stronger than 7B or 13B models:</p> </li> <li> <p>More coherent multi-turn conversations.</p> </li> <li>Better code generation &amp; debugging.</li> <li>More accurate reasoning over long documents.</li> <li>Handles longer context windows (if supported by quantization/runtime).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#6-enterprise-research-use","title":"\ud83d\udcc8 6. Enterprise &amp; Research Use","text":"<ul> <li>You can deploy it as an internal assistant in small companies without exposing data to OpenAI/Anthropic/Google.</li> <li>Academics and students can use it for NLP research, AI experiments, or prototyping without cloud restrictions.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#what-can-you-achieve-with-gpt-oss20b-locally","title":"\ud83c\udfaf What Can You Achieve With GPT-OSS:20B Locally?","text":"<p>Here are some practical, concrete goals you could accomplish:</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#productivity-knowledge-work","title":"\ud83d\udd39 Productivity &amp; Knowledge Work","text":"<ul> <li>Summarize large PDFs, books, or technical documents.</li> <li>Draft reports, proposals, or knowledge-base articles.</li> <li>Answer research queries from your local corpus (RAG setup).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#programming-devops","title":"\ud83d\udd39 Programming &amp; DevOps","text":"<ul> <li>Code generation, debugging, and explanations.</li> <li>Local copilots for VS Code, JetBrains, etc.</li> <li>Infrastructure automation with natural language commands (shell + Ansible + Dockerfile generation).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#research-experimentation","title":"\ud83d\udd39 Research &amp; Experimentation","text":"<ul> <li>Fine-tune models on niche data (medical, law, customer service).</li> <li>Evaluate and benchmark new quantization methods.</li> <li>Compare with other OSS models (Llama 2, Mistral, Mixtral).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#private-ai-agents","title":"\ud83d\udd39 Private AI Agents","text":"<ul> <li>Build local chatbots, assistants, or role-playing AIs without external API calls.</li> <li>Integrate with local tools (e.g., calendar, email, files) without data leaving your machine.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#creative-work","title":"\ud83d\udd39 Creative Work","text":"<ul> <li>Generate stories, scripts, and world-building content.</li> <li>Brainstorm new ideas privately (no IP leaks).</li> <li>Assist with language learning or translation.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#pros-of-local-gpt-oss20b-vs-cloud-api","title":"\ud83d\udfe2 Pros of Local GPT-OSS:20B vs Cloud API","text":"<ul> <li>\u2705 Privacy (your data never leaves your machine).</li> <li>\u2705 No API fees.</li> <li>\u2705 Always available, even offline.</li> <li>\u2705 Full customization (quantization, finetuning, RAG integration).</li> <li>\u2705 Model weights are open \u2014 no black-box restrictions.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#cons","title":"\ud83d\udd34 Cons","text":"<ul> <li>\u274c Requires high-end hardware (VRAM + RAM).</li> <li>\u274c Slower than GPT-4-level APIs \u2014 20B \u2248 GPT-3.5 quality, not SOTA.</li> <li>\u274c Energy use (desktop GPUs draw 300\u2013400W).</li> <li>\u274c Setup requires technical comfort (install runtimes, drivers, quantized weights).</li> </ul> <p>\u2728 Bottom line: Running GPT-OSS:20B locally is about independence, privacy, cost savings, and control. It won\u2019t beat GPT-4 or Claude 3.5 in raw quality, but it gives you a serious personal/private AI lab that you can shape to your own needs.</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#budget-option-a-balanced-budget-recommended-for-stability","title":"Budget Option A \u2014 Balanced Budget (recommended for stability)","text":"<p>Estimated total: \u2248 $2,000</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#parts-exacttypical-skus","title":"Parts (exact/typical SKUs)","text":"<ul> <li>CPU: AMD Ryzen 7 7700X \u2014 8c/16t, strong single-thread. \u2014 $320</li> <li>Motherboard: B650 ATX (e.g., MSI B650 Tomahawk or ASUS TUF B650-PLUS) \u2014 AM5, DDR5 support \u2014 $160</li> <li>GPU: AMD Radeon RX 7900 XT (20GB) \u2014 20 GB VRAM (sufficient for 4-bit 20B). Prefer new from ASUS/Sapphire/MSI \u2014 $850\u2013$950</li> <li>RAM: 64 GB (2\u00d732 GB) DDR5-5600/6000 \u2014 DDR5-5600/6000 CL32 \u2014 $180\u2013$260 Note: For strict budget you can choose 32 GB (2\u00d716) now and upgrade later; but 64 GB is safer.</li> <li>Primary NVMe: 1 TB PCIe4 NVMe (Samsung 990 Pro or WD SN850) \u2014 $110\u2013$140</li> <li>PSU: 850 W 80+ Gold (Corsair RM850x / Seasonic Focus GX) \u2014 $100\u2013$150</li> <li>CPU Cooler: 240 mm AIO (NZXT Kraken X53 / Corsair H100) or quality air cooler \u2014 $80\u2013$120</li> <li>Case: Good airflow mid-tower (Fractal Meshify, Phanteks P400A) \u2014 $70\u2013$120</li> <li>Misc (fans, cables, thermal paste): $30\u2013$60</li> </ul> <p>Estimated subtotal: $1,900 (with 32GB RAM) \u2192 $2,050 (with 64GB RAM). Savings vs recommended (~$2,840 parts-only): ~$790\u2013$940 (meets your $800\u2013$1,200 target).</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#why-this-works","title":"Why this works","text":"<ul> <li>RX 7900 XT (20GB) has enough VRAM to hold a 20B model quantized to 4-bit (\u224810\u201312 GB) with room for GPU buffers.</li> <li>Ryzen 7 7700X (8c/16t) is still very capable for token preprocessing and feeding GPU.</li> <li>64 GB RAM ideal; 32 GB will still run 4-bit 20B but leaves less headroom for other apps \u2014 recommended to aim for 64 if budget allows.</li> <li>PCIe4 NVMe ensures quick model load times without needing PCIe5.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#expected-performance-very-approximate","title":"Expected performance (very approximate)","text":"<ul> <li>GPT-OSS 20B (4-bit) on RX 7900 XT \u2192 ~3\u20136 tokens/sec (depends on runtime, quantization, threading).</li> <li>Smaller models (7B/13B) will be much faster (10\u201330 t/s).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#budget-option-b-tighter-value-build-max-savings-used-gpu","title":"Budget Option B \u2014 Tighter Value Build (max savings, used GPU)","text":"<p>Target: aggressive cost reduction while still enabling 4-bit 20B. Use a used GPU (good deal hunting required).</p> <p>Estimated total: \u2248 $1,400 \u2013 $1,600</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#parts-exacttypical-skus_1","title":"Parts (exact/typical SKUs)","text":"<ul> <li>CPU: Ryzen 5 7600X (6c/12t) \u2014 $230</li> <li>Motherboard: B650 or B650M \u2014 $120\u2013$150</li> <li>GPU (used): AMD Radeon RX 6900 XT (16GB) used \u2014 $450\u2013$650 (used market)   Alternative used: RX 7900 XT if you can find one for ~$700\u2013$850 \u2014 better choice if available</li> <li>RAM: 32 GB (2\u00d716) DDR5-5600/6000 \u2014 $120\u2013$160</li> <li>Primary NVMe: 1 TB PCIe4 NVMe \u2014 $100\u2013$120</li> <li>PSU: 850 W 80+ Gold \u2014 $100</li> <li>CPU Cooler: solid air cooler (Noctua NH-U12S) or 240 AIO \u2014 $60\u2013$100</li> <li>Case + misc: $80\u2013$100</li> </ul> <p>Estimated subtotal: $1,400\u2013$1,600 Savings vs recommended (~$2,840): ~$1,200\u2013$1,440</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#tradeoffs-and-notes","title":"Tradeoffs and notes","text":"<ul> <li>RX 6900 XT (16 GB): 16 GB VRAM is enough for 4-bit 20B (~10\u201312GB), but leaves less VRAM room for multi-buffers or memory fragmentation. You must ensure model + runtime buffers fit. Some runtimes may need extra tuning.</li> <li>Ryzen 5 7600X has fewer cores \u2014 will slightly limit preprocessing throughput; CPU may become a mild bottleneck if heavy multitasking.</li> <li>Used GPU risks: warranty, unknown usage, driver quirks. Buy from reputable sellers with returns.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#expected-performance-approx","title":"Expected performance (approx)","text":"<ul> <li>With RX 6900 XT (16GB): ~2\u20134 tokens/sec for 20B (4-bit), depending on runtime and optimizations. Possibly slower than RX 7900 XT.</li> <li>If you find a used RX 7900 XT (~$700\u2013$850), expect similar performance to Balanced Budget (~3\u20136 t/s).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#what-to-upgrade-later-if-you-start-with-tighter-build","title":"What to upgrade later (if you start with tighter build)","text":"<ol> <li>RAM: upgrade from 32 \u2192 64 GB when budget allows (biggest single uplift for stability).</li> <li>GPU: replace used 6900 XT with RX 7900 XT/XTX for faster tokens/sec and more VRAM headroom.</li> <li>NVMe capacity: add a second NVMe for datasets/models to avoid juggling files.</li> </ol>"},{"location":"ai-pc/pc-for-running-20b-llm/#software-runtime-tips-for-both-builds","title":"Software &amp; runtime tips for both builds","text":"<ul> <li>Use llama.cpp with Vulkan backend or a runtime that supports AMD GPU acceleration (Vulkan / ROCm where available).</li> <li>Prefer 4-bit GGUF/GPTQ quantized models for lowest memory footprint.</li> <li>Tune threads (CPU) and <code>n_batch</code>/<code>n_ctx</code> to balance latency vs throughput.</li> <li>Monitor VRAM usage; if model doesn\u2019t fit, try a 4-bit variant with GPTQ/GGUF.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#shopping-tips-cautions","title":"Shopping tips &amp; cautions","text":"<ul> <li>GPU prices fluctuate \u2014 check Newegg, Amazon, local sellers, and reputable used marketplaces (e.g., eBay with buyer protection).</li> <li>If buying used, prefer sellers with return windows and verify photos/serial if possible.</li> <li>RAM timing: buy a tested DDR5 kit (same dual-rank sticks), avoid mixing kits.</li> <li>PSU quality matters \u2014 don\u2019t buy the cheapest. Good PSU protects your expensive GPU/CPU.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#quick-summary-comparison","title":"Quick summary comparison","text":"Build Total est. Savings vs recommended VRAM RAM Perf (20B 4-bit) Balanced Budget (7900 XT + 64GB) $1,900\u2013$2,050 ~$800\u2013$940 20GB 64GB ~3\u20136 t/s Tighter Value (used 6900 XT + 32GB) $1,400\u2013$1,600 ~$1,200\u2013$1,440 16GB (used) 32GB ~2\u20134 t/s"},{"location":"ai-pc/pc-for-running-20b-llm/#final-recommendation","title":"Final recommendation","text":"<ul> <li>If you want reliable interactive use and longevity, go with Balanced Budget (Option A) \u2014 it meets your $800\u2013$1,200 savings target and keeps good headroom.</li> <li>If your goal is maximum savings now and you\u2019re comfortable risk-managing used parts and future upgrades, use Option B and plan to upgrade RAM/GPU later.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#why-ryzen-9-7950x-rx-7900-xtx-64-gb-updated-memory-pcie-ssd","title":"Why Ryzen 9 7950X + RX 7900 XTX + 64 GB (updated: memory, PCIe, SSD)","text":""},{"location":"ai-pc/pc-for-running-20b-llm/#quick-summary-of-the-new-details","title":"Quick summary of the new details","text":"<ul> <li>RAM type: DDR5 is recommended for Ryzen 9 7950X (speed and Infinity Fabric behavior).</li> <li>RAM speed: Aim for DDR5-6000 MT/s (or 5600\u20136400 MT/s) CL30\u201336 for best latency/throughput balance on Zen4.</li> <li>PCIe: PCIe 4.0 x16 is sufficient for a GPU like RX 7900 XTX; PCIe 5.0 is beneficial for NVMe SSDs and future GPUs but not required for GPU performance today.</li> <li>NVMe SSD: Use PCIe 4.0 NVMe with sustained sequential reads ~5\u20137 GB/s (5000\u20137000 MB/s). If you want the fastest possible model load times and you have a PCIe5 motherboard, PCIe 5.0 NVMe drives (read ~10\u201312+ GB/s) are an extra boost for model I/O.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#why-these-choices-matter-technical-view","title":"Why these choices matter (technical view)","text":""},{"location":"ai-pc/pc-for-running-20b-llm/#ddr5-vs-ddr4-and-speed","title":"DDR5 vs DDR4 (and speed)","text":"<ul> <li> <p>Ryzen 9 7950X (Zen4):</p> </li> <li> <p>Designed for DDR5. DDR5 gives higher bandwidth which helps when the CPU is moving large model chunks and when the system uses RAM as staging for GPU transfers.</p> </li> <li>Recommended speed: DDR5-6000 MT/s is a well-known sweet spot for Ryzen 7000-series \u2014 it often allows Infinity Fabric to run 1:1 (better latency) and yields the best practical throughput. DDR5-5600 \u2192 good, DDR5-6400 \u2192 sometimes faster but may need manual tuning/timings.</li> <li>Timings: lower CAS latency helps, but MT/s matters more for AI workloads (bandwidth-bound). Typical good kit: 6000 MT/s CL30\u201336.</li> <li> <p>If you choose Ryzen 5000 (5900X):</p> </li> <li> <p>DDR4-3600 CL16 is ideal. DDR5 is not supported by that platform.</p> </li> <li> <p>Why not DDR4 on 7950X?</p> </li> <li> <p>7950X motherboards support DDR5 only; DDR4 is incompatible. For other CPUs, DDR4 is fine but lower bandwidth.</p> </li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#capacity-64-gb-vs-32-gb","title":"Capacity (64 GB vs 32 GB)","text":"<ul> <li> <p>64 GB gives headroom for:</p> </li> <li> <p>FP16/8-bit weights and runtime buffers</p> </li> <li>caching multiple models, datasets, or running containers/IDE alongside inference</li> <li>avoiding swap (swap kills throughput)</li> <li>32 GB may be okay for 4-bit 20B but is tight and risky.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#pcie-40-vs-pcie-50","title":"PCIe 4.0 vs PCIe 5.0","text":"<ul> <li>GPU: RX 7900 XTX runs perfectly on PCIe 4.0 x16. Moving to PCIe 5.0 gives little-to-no real-world inference speedup today because the GPU computation is the bottleneck, not PCIe link bandwidth for steady-state inference.</li> <li>NVMe SSDs: Where PCIe5 shines \u2014 PCIe 5.0 NVMe offers ~10\u201314 GB/s sequential reads and speeds up model load times and swap-backed workloads significantly. If you often load many model checkpoints or large datasets, PCIe5 NVMe helps.</li> <li>Practical: Buy a good PCIe4 NVMe unless you specifically want the fastest possible model load times and have a PCIe5 board.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#nvme-ssd-readwrite-what-to-pick","title":"NVMe SSD Read/Write (what to pick)","text":"<ul> <li> <p>Good target for model work:</p> </li> <li> <p>PCIe4 NVMe: Sequential read 5,000\u20137,000 MB/s; write similar for high-end drives. Examples: Samsung 990 Pro, WD Black SN850 \u2013 fast and reliable.</p> </li> <li>PCIe5 NVMe: Sequential read 10,000\u201314,000 MB/s; cutting-edge (requires PCIe5 motherboard).</li> <li>Why sequential read matters: Model files are large sequential reads when loading weights; higher read speeds reduce model startup time and any disk-backed paging overhead.</li> <li>IOPS &amp; endurance: For cache-heavy usage, consider high IOPS and higher TBW (endurance) ratings if you will swap or write a lot.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#updated-comparison-table-includes-ddr-pcie-ssd-speeds","title":"Updated comparison table (includes DDR, PCIe, SSD speeds)","text":"Build CPU GPU RAM (type &amp; speed) RAM capacity PCIe NVMe suggestion (read/write) Notes &amp; expected t/s Recommended Ryzen 9 7950X (16c/32t) RX 7900 XTX (24 GB) DDR5-6000 MT/s (CL30-36) 64 GB PCIe 4.0 x16 (PCIe5 ready mobo optional) PCIe4 NVMe: 5\u20137 GB/s (PCIe5: 10\u201314 GB/s) Best balance; smooth FP16/INT8 20B inference (~6\u201312 t/s) High-end pro Threadripper 3965WX Radeon PRO W6800 (32GB) DDR4/DDR5 ECC (platform dependent) 128 GB+ PCIe4/PCIe5 server board NVMe PCIe4/5: 7\u201312+ GB/s Multi-model / heavy fine-tuning (10\u201320 t/s) Mid / cost-aware Ryzen 9 5900X (12c/24t) RX 7900 XT/XTX DDR4-3600 (if 5900X) or DDR5 on other CPUs 32\u201364 GB (64 preferred) PCIe 4.0 PCIe4 NVMe: 5\u20137 GB/s Good value; upgrade RAM to 64GB for FP16 20B (~4\u20139 t/s) Budget Ryzen 7 7700X / 5800X3D RX 7900 XT or used 6900 XT DDR5-5200\u20135600 (for 7700X) or DDR4-3600 32 GB PCIe4 PCIe4 NVMe: ~5 GB/s Can run quantized 20B (4-bit), slower (~3\u20136 t/s) Laptop / small Ryzen 9 8945HS Radeon 780M (iGPU) DDR5 LPDDR5x (integrated) 32 GB \u2014 NVMe PCIe4: 5 GB/s Can run 4-bit 20B but mostly CPU-bound (&lt;1\u20132 t/s)"},{"location":"ai-pc/pc-for-running-20b-llm/#concrete-component-recommendations","title":"Concrete component recommendations","text":"<ul> <li> <p>RAM (Ryzen 7950X combo):</p> </li> <li> <p>Buy 2\u00d732 GB DDR5-6000 CL30 (dual-channel) or 4\u00d716 GB if motherboard supports quad. Good kits: reputable brands (G.Skill Trident Z5, Corsair Dominator DDR5).</p> </li> <li>On Ryzen 7000, DDR5-6000 often hits a good Infinity Fabric 1:1 ratio.</li> <li> <p>SSD:</p> </li> <li> <p>Primary NVMe (OS + models): 1\u20132 TB PCIe 4.0 NVMe (Samsung 990 Pro, WD SN850) \u2014 read ~7000 MB/s.</p> </li> <li>Optional ultra-fast scratch: If you have PCIe5 board, consider a PCIe5 drive for super-fast model loads.</li> <li> <p>Motherboard:</p> </li> <li> <p>X670E/B650E for Ryzen 7000 \u2013 choose one with multiple M.2 NVMe slots and PCIe 4.0/5.0 support if future-proofing.</p> </li> <li> <p>PSU &amp; Cooling:</p> </li> <li> <p>PSU: 850\u20131000 W high-quality Gold/Plat</p> </li> <li>Cooling: 240\u2013360 mm AIO for 7950X sustained loads</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#practical-tuning-tips","title":"Practical tuning tips","text":"<ul> <li>Memory config: Use dual-channel kits (2 sticks) for simplicity; 64 GB as 2\u00d732 GB is ideal. Populate recommended slots per manual.</li> <li>RAM speed tuning: Set XMP/EXPO to kit speed (6000) and verify FCLK (Infinity Fabric) settings \u2014 many motherboards support 1:1 at 6000.</li> <li>SSD configuration: Put models on the fastest NVMe drive. If using swap, use a fast NVMe with high TBW.</li> <li>PCIe lanes: Make sure the GPU gets full x16 lanes (most desktop boards give this to GPU slot).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#short-checklist-when-buyingbuilding-for-gpt-oss-20b","title":"Short checklist when buying/building for GPT-OSS 20B","text":"<ol> <li>CPU: Ryzen 9 7950X or equivalent desktop-class CPU (&gt;=12 cores recommended).</li> <li>GPU: Discrete GPU with \u226524 GB VRAM (RX 7900 XTX or equivalent).</li> <li>RAM: 64 GB DDR5 (DDR5-6000 kit recommended).</li> <li>Storage: PCIe4 NVMe (1\u20132 TB) \u2014 PCIe5 optional for fastest loads.</li> <li>Motherboard: X670E/B650E with multiple M.2 slots.</li> <li>PSU/Cooling: 850\u20131000W PSU, quality AIO cooler.</li> </ol>"},{"location":"ai-sessions/fundamentals_of_LLM/","title":"\ud83e\udd16 Fundamentals of LLM","text":""},{"location":"ai-sessions/fundamentals_of_LLM/#video","title":"\ud83d\udcf9Video","text":"<p>Please find the video of the session on fundamentals of LLM(6th Sep 2025) here- https://drive.google.com/file/d/1Q3N9WjYwNyYyz0Zx120ODoU22ZeswnYN/view?usp=drive_link.</p>"},{"location":"ai-sessions/fundamentals_of_LLM/#_1","title":"AI Fundamentals 1","text":""},{"location":"ai-sessions/fundamentals_of_LLM/#curated-lists","title":"\ud83d\uddc2\ufe0f Curated Lists","text":"<ol> <li>https://poloclub.github.io/transformer-explainer/ - Transformer Playground</li> <li>https://huggingface.co/spaces/ShreyaL/NLP_preprocessing_playground?logs=container \u2013 Text Pre-processing Playground</li> <li>https://huggingface.co/spaces/Xenova/the-tokenizer-playground - Tokenizer Playground</li> <li>https://www.promptingguide.ai/techniques/cot - Guide for Prompting</li> <li>https://jalammar.github.io/illustrated-transformer/ - Visual illustration of Transformer</li> <li>https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/ - Short Free Course on Prompt Engineering</li> <li>https://huggingface.co/blog/getting-started-with-embeddings - Embeddings article </li> </ol>"},{"location":"ai-sessions/fundamentals_of_LLM/#_2","title":"AI Fundamentals 1","text":""},{"location":"ai-sessions/fundamentals_of_LLM/#request-for-like","title":"\ud83d\udc4dRequest for Like","text":"<p>If you like this app https://huggingface.co/spaces/ShreyaL/NLP_preprocessing_playground?logs=container \u2013 Text Pre-processing Playground and find it useful while learning, please give a heart in the huggingface spaces using above link. Will be very greatful for your support.</p>"},{"location":"ai-sessions/fundamentals_of_LLM/#_3","title":"AI Fundamentals 1","text":""},{"location":"ai-sessions/fundamentals_of_LLM/#thank-you","title":"\ud83d\ude4fThank You!","text":""},{"location":"ai-tools-hands-on/AccessWorldOfFreeLLMs-guff/How-to-freely-access-use-LLMs-locally-securely-with-no-limits/","title":"Access world of free LLMs Demo","text":"<p>Video Demo</p>"},{"location":"ai-tools-hands-on/AccessWorldOfFreeLLMs-guff/How-to-freely-access-use-LLMs-locally-securely-with-no-limits/#more-about-single-file-llms","title":"More about Single File LLMs","text":""},{"location":"ai-tools-hands-on/AccessWorldOfFreeLLMs-guff/How-to-freely-access-use-LLMs-locally-securely-with-no-limits/#guff-formats","title":"GUFF Formats","text":""},{"location":"ai-tools-hands-on/AccessWorldOfFreeLLMs-guff/How-to-freely-access-use-LLMs-locally-securely-with-no-limits/#pretext-what-is-ggml","title":"Pretext: What is GGML?","text":"<p>GGML was the direct predecessor to GGUF. Its name stands for \"Georgi Gerganov Machine Learning,\" named after its creator and the <code>llama.cpp</code> project. It was a groundbreaking tensor library that enabled running large language models on standard CPUs with great efficiency.</p> <p>While it was highly effective, it had some limitations: * Lack of Standardization: The format evolved quickly, and changes often broke compatibility with older models. * Incomplete Metadata: It didn't store all the necessary information in the file itself. This meant that for many models, you still needed separate JSON configuration files to run them.</p> <p>GGUF was created to solve these problems. It is a new, more robust format built on the principles of GGML. It's a single, self-contained file that includes all the model's weights, tokenizer, and metadata, making it easier to share and use.  GGUF is now the preferred format for local CPU/hybrid inference.</p>"},{"location":"ai-tools-hands-on/AccessWorldOfFreeLLMs-guff/How-to-freely-access-use-LLMs-locally-securely-with-no-limits/#what-is-gguf-format","title":"What is GGUF format","text":"<p>GGUF (GPT-Generated Unified Format) is a single-file format designed to make it easy and efficient to run large language models on consumer hardware, particularly using a CPU or a mix of CPU and GPU. It's a binary format that bundles the model's weights, its architecture, and the tokenizer into one file, so you don't have to manage multiple files. Its main strength is efficient memory usage and support for various levels of quantization, which reduces the model's file size.</p> <p>Popular GGUF Examples: Llama 3 (8B and 70B), Mixtral 8x7B, and many fine-tuned models from creators like \"TheBloke\" on Hugging Face.</p>"},{"location":"ai-tools-hands-on/AccessWorldOfFreeLLMs-guff/How-to-freely-access-use-LLMs-locally-securely-with-no-limits/#comparison-of-gguf-with-other-model-formats","title":"Comparison of GGUF with Other Model Formats","text":"Format Purpose &amp; Best Use Case Popular Examples Key Advantage GGUF Inference on consumer hardware. Optimized for local use via tools like <code>llama.cpp</code> and Ollama. Llama 3 GGUF, Mixtral 8x7B GGUF (from TheBloke). Accessibility: Runs well on most computers, even without a high-end GPU. PyTorch / TensorFlow Training and Research. The original formats used by developers for fine-tuning. Meta Llama 3, Mistral 7B, Gemma 2B (in their original PyTorch format). Flexibility: The standard for fine-tuning and development, offering full control. SafeTensors Secure Storage of model weights. Great for sharing models safely on platforms like Hugging Face. Stable Diffusion models, like DreamShaper and Realistic Vision. Safety: Prevents malicious code from executing when you load the model. GPTQ, AWQ, EXL2 Inference with extreme efficiency, but typically GPU-only. Mixtral 8x7B GPTQ, Llama 2 AWQ. Speed: Best performance and lowest VRAM usage if the model fits entirely on the GPU. MXFP4 Inference on consumer hardware, a specific quantization format for MoE models. <code>oss-gpt-20b</code> (natively supported by Ollama). Efficiency: Highly optimized for running Mixture-of-Experts (MoE) models with reduced memory footprint."},{"location":"ai-tools-hands-on/AccessWorldOfFreeLLMs-guff/How-to-freely-access-use-LLMs-locally-securely-with-no-limits/#ollama-for-pulling-guff-models","title":"Ollama for pulling GUFF models","text":"<p>Ollama's model library, which you access with <code>ollama pull &lt;model_name&gt;</code>, is built on top of GGUF models. When you pull a model from the Ollama library, it's essentially downloading a GGUF file and its associated manifest and configuration files. This process is handled seamlessly by the <code>ollama</code> CLI. </p> <p>However, you can also use Ollama to run any GGUF model you've downloaded from other sources, like Hugging Face, by using a <code>Modelfile</code>. This gives you the flexibility to use models not yet in the official Ollama library.</p>"},{"location":"ai-tools-hands-on/AccessWorldOfFreeLLMs-guff/How-to-freely-access-use-LLMs-locally-securely-with-no-limits/#filtering-gguf-models-on-hugging-face","title":"Filtering GGUF Models on Hugging Face \ud83d\udd0e","text":"<p>To filter for GGUF models on Hugging Face, you can use the built-in search filters. The easiest way is to use the <code>library:gguf</code> filter directly in the search bar. This will show you all models that have GGUF files associated with them.</p> <p>Here's how to do it:</p> <ol> <li>Go to the Hugging Face Models page: <code>huggingface.co/models</code>.</li> <li>In the search bar, type <code>library:gguf</code> and press Enter.</li> </ol> <p>Alternatively, you can navigate to the Libraries filter on the left-hand side of the page and select \"GGUF\" from the list.</p> <p>Many popular models on Hugging Face, especially those from creators like TheBloke, are offered in the GGUF format, often with different quantization levels (e.g., Q4_K_M, Q8_0). These are ready to be used with tools like Ollama and <code>llama.cpp</code>.</p> <p>Example command:</p> <pre><code>ollama pull hf.co/asmaeag/llama-finetuned-nutrition-v2\n</code></pre>"},{"location":"ai-tools-hands-on/CuratedLocalLLMs/Top-Local-LLMs/","title":"Top Local LLMs","text":""},{"location":"ai-tools-hands-on/CuratedLocalLLMs/Top-Local-LLMs/#top-free-small-language-models-slm-comparison","title":"Top Free Small Language Models (SLM) Comparison","text":"Model Name (Repository) Family/Developer Parameter Size License Key Capabilities (Good At) Validation Gemma 3 - 270M Google / Gemma 270 Million Gemma License Question answering, summarization, and reasoning. Confirmed. Excellent performance for its compact size, especially in common NLP tasks. Qwen3 - 0.6B Alibaba / Qwen 0.6 Billion (600M) Tongyi Qianwen License Switch between \u201cthinking mode\u201d for complex reasoning, math, and coding, and \u201cnon-thinking mode\u201d for fast, general-purpose dialogue. Confirmed. Designed for fast general-purpose dialogue and complex problem-solving in a compact model. SmolLM3 - 3B HuggingFaceTB 3 Billion Apache 2.0 Thinking more for complex problem solving, agentic usage with tool calling, making it versatile for real-world applications. Confirmed. The model is specifically tuned for agentic workflows and complex problem-solving. Qwen3 - 4B-Instruct-2507 Alibaba / Qwen 4 Billion Tongyi Qianwen License Logical reasoning, text comprehension, mathematics, science, coding, and tool usage. Confirmed. This is an instruction-tuned variant optimized for a wide range of academic, reasoning, and coding benchmarks. Gemma 3 - 4B-IT Google / Gemma 4 Billion Gemma License Fine-tuning on text classification, image classification, &amp; specialized tasks. Confirmed. The Instruction-Tuned (<code>-it</code>) variant is optimized for instruction-following and serves as a strong base for further fine-tuning on specialized tasks. Granite 4 (tiny) granite4:tiny-h IBM / Granite 4.0 7 Billion Apache 2.0 Question answering, summarization, classification, code answering, function calling, RAG, and multilingual support. Confirmed. The Granite 4.0-Micro is a 3B model under Apache 2.0, optimized for enterprise tasks like RAG, function calling, and multilingual support."},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/Git-hub-Sersion-Step-by-step-guide-to-setup-n8n-locally-with-llm/","title":"n8n - Step-by-step setup guide with Docker Desktop","text":""},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/Git-hub-Sersion-Step-by-step-guide-to-setup-n8n-locally-with-llm/#demo","title":"Demo","text":""},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/Git-hub-Sersion-Step-by-step-guide-to-setup-n8n-locally-with-llm/#steps","title":"steps","text":"<ol> <li> <p>Download and install docker desktop</p> <p>Note: Its better install with recommended options like wsl</p> <p></p> <p></p> <p>Info: If you installed with default options, the below wsl window will open, you can close it safely.</p> <p></p> </li> <li> <p>After installion is complete, restart might be needed, then lookup \"Docker Desktop\" in windows start meny and open it.</p> <p></p> </li> <li> <p>CLick Search on top and type n8n in seach window, and then click pull for n8n image</p> <p></p> <p></p> <p></p> </li> <li> <p>Run the container and provide the parameters and values shown in screenshot below</p> <p></p> <p></p> <p></p> </li> <li> <p>URL will be generated, open it in browser and create a free account</p> <p></p> <p></p> </li> <li> <p>On home page, click on create workflow to see workflow screen.</p> <p></p> </li> <li> <p>We'll take a step back to add community nodes for n8n</p> <p></p> <p></p> <p></p> </li> <li> <p>Open the new workflow again, search for triggers and add chat trigger, nothing to modify in settings, click back.</p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Now lets look for AI agent and add it, no setting to modify for now, click back and agent should be added with nodes for chat model, memory and tool.</p> <p></p> <p></p> <p></p> </li> <li> <p>Now lets click on chat model node of the agent and search for Ollama chat model , selct it, in config screen, click add credential and enter docker url shown, no api key needed as it local LLM, connection will be tested and click close and select the LLM.NOTE you might need to reopen to see LLM sometimes.</p> <p></p> <p></p> <p></p> </li> <li> <p>Click on memory of the AI agent and add simple memory node.</p> <p> </p> </li> </ol>"},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/Step-by-step-guide-to-setup-n8n-locally-with-llm/","title":"n8n - Step-by-step setup guide with Docker Desktop","text":""},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/Step-by-step-guide-to-setup-n8n-locally-with-llm/#demo","title":"Demo","text":""},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/Step-by-step-guide-to-setup-n8n-locally-with-llm/#steps","title":"steps","text":"<ol> <li> <p>Download and install docker desktop</p> <p>Note: Its better install with recommended options like wsl</p> <p></p> <p></p> <p>Info: If you installed with default options, the below wsl window will open, you can close it safely.</p> <p></p> </li> <li> <p>After installion is complete, restart might be needed, then lookup \"Docker Desktop\" in windows start meny and open it.</p> <p></p> </li> <li> <p>CLick Search on top and type n8n in seach window, and then click pull for n8n image</p> <p></p> <p></p> <p></p> </li> <li> <p>Run the container and provide the parameters and values shown in screenshot below</p> <p></p> <p></p> <p></p> </li> <li> <p>URL will be generated, open it in browser and create a free account</p> <p></p> <p></p> </li> <li> <p>On home page, click on create workflow to see workflow screen.</p> <p></p> </li> <li> <p>We'll take a step back to add community nodes for n8n</p> <p></p> <p></p> <p></p> </li> <li> <p>Open the new workflow again, search for triggers and add chat trigger, nothing to modify in settings, click back.</p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Now lets look for AI agent and add it, no setting to modify for now, click back and agent should be added with nodes for chat model, memory and tool.</p> <p></p> <p></p> <p></p> </li> <li> <p>Now lets click on chat model node of the agent and search for Ollama chat model , selct it, in config screen, click add credential and enter docker url shown, no api key needed as it local LLM, connection will be tested and click close and select the LLM.NOTE you might need to reopen to see LLM sometimes.</p> <p></p> <p></p> <p></p> </li> <li> <p>Click on memory of the AI agent and add simple memory node.</p> <p> </p> </li> </ol>"},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/","title":"Prviate-Secure Docker-Compose n8n + local LLM","text":""},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#n8n-one-step-deploy-on-docker-with-docker-compose","title":"n8n one step deploy on docker with docker-compose","text":""},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#run-with-docker-compose","title":"run with docker compose","text":""},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step-1-open-your-terminal-or-command-prompt","title":"Step 1: Open Your Terminal or Command Prompt","text":"<p>First, you need to open a terminal (on Linux or macOS) or a command prompt/PowerShell (on Windows).</p>"},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step-2-navigate-to-the-files-directory","title":"Step 2: Navigate to the File's Directory","text":"<p>Using the <code>cd</code> (change directory) command, navigate to the folder where you saved your <code>docker-compose.yml</code> file. For example, if you saved the file in a folder called <code>n8n-project</code> on your desktop, you would use a command like this:</p> <p><code>cd ~/Desktop/n8n-project</code></p>"},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#create-a-file-on-your-machine-as-docker-composeyml-below-content","title":"create a file on your machine as 'docker-compose.yml' below content","text":"<pre><code>\nservices:\n  n8n:\n    image: n8nio/n8n:latest\n    container_name: n8n\n    restart: unless-stopped\n    ports:\n      - \"5678:5678\"\n    volumes:\n      - n8n_data:/home/node/.n8n\n    environment:\n      - N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE=true\n      # No need for N8N_LLM_SERVER_URL for Ollama integration &gt;&gt;       http://host.docker.internal:11434\n      - N8N_LLM_SERVER_URL=http://ollama:11434\n    depends_on:\n      - ollama\n    networks:\n      - n8n-ollama-network\n\n\n  ollama:\n    image: ollama/ollama:latest\n    container_name: ollama\n    restart: unless-stopped\n    ports:\n      - \"11434:11434\"  # Expose Ollama on the default port\n    volumes:\n      - ollama_data:/root/.ollama\n\n    networks:\n      - n8n-ollama-network\n\nvolumes:\n  n8n_data:\n  ollama_data:\n\nnetworks:\n  n8n-ollama-network:\n\n</code></pre>"},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step-3-run-the-docker-compose-command","title":"Step 3: Run the Docker Compose Command","text":"<p>Once you are in the correct directory, you can use the following command to start n8n. This command tells Docker Compose to read the <code>docker-compose.yml</code> file and start the services defined within it.</p> <p><code>docker compose up -d</code></p> <ul> <li><code>docker compose up</code>: This part of the command initiates the process of creating and starting the containers.</li> <li><code>-d</code>: This flag stands for \"detached mode.\" It runs the container in the background, so you can continue to use your terminal for other tasks without keeping it open to manage the running container.</li> </ul>"},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step-4-access-n8n","title":"Step 4: Access N8N","text":"<p>After running the command, Docker will download the n8n image (if it's not already on your system) and start the container. You can then access the n8n web interface by opening your web browser and navigating to <code>http://localhost:5678</code>.</p>"},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#local-llms","title":"Local LLMs","text":"<pre><code>$ docker exec -ti ollama /bin/bash\nroot@b65cb311556b:/# ollama pull llama3.2\n</code></pre> <p>Several other GGUF models support tool use and function calling, which are essential for building agents that can interact with external APIs and services. The ability to use tools is a rapidly evolving area in the world of open-source LLMs.</p>"},{"location":"ai-workflows/n8n/Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step5-login-and-create-workflows","title":"Step5: Login and create workflows","text":"<p>Follow the steps from Step 5 onwards in docker-desktop-based-setup</p>"},{"location":"community-values-guidelines/Prompt-answer/","title":"A Chat GTP resposne","text":""},{"location":"community-values-guidelines/Prompt-answer/#the-foundation-community-driven-selfless-efforts","title":"The Foundation: Community-Driven, Selfless Efforts","text":""},{"location":"community-values-guidelines/Prompt-answer/#in-one-line-from-chatgpt","title":"In one line from ChatGPT","text":"<p>The culture of open sharing, learning, and collaboration is the engine of progress in AI. Proprietary tools are the vehicles that make that progress practical, accessible, and sustainable. But without the engine, the vehicle goes nowhere. promotions, tool referrals, or marketing content mainly drive usage and sales. They don\u2019t advance the science itself\u2014they ride on the wave of what community-driven research has already created.</p> <ul> <li> <p>Open-Source Code &amp; Libraries</p> </li> <li> <p>Frameworks like TensorFlow, PyTorch, Hugging Face Transformers, scikit-learn and even Linux/Ubuntu itself are open-source. Without them, no AI researcher or startup could move quickly.</p> </li> <li> <p>Community-driven bug fixes, optimizations, and features keep them robust and usable.</p> </li> <li> <p>Shared Research &amp; Publications</p> </li> <li> <p>Papers like \u201cAttention is All You Need\u201d (transformers), BERT, and diffusion models were published openly, not hidden behind paywalls.</p> </li> <li> <p>Preprints on arXiv and public leaderboards (Kaggle, PapersWithCode) democratize knowledge.</p> </li> <li> <p>Tutorials, Guides, and Free Education</p> </li> <li> <p>Countless blog posts, YouTube tutorials, MOOCs (fast.ai, MIT OpenCourseWare), and GitHub repos taught developers how to build and understand models.</p> </li> <li> <p>This culture of teaching lowers the barrier for new learners\u2014someone in a small town with just an internet connection can join the global AI wave.</p> </li> <li> <p>Collaboration and Peer-Learning</p> </li> <li> <p>Stack Overflow, Reddit, Discord, and open forums provide real-time mentorship and troubleshooting.</p> </li> <li>Hackathons, community challenges, and shared datasets (ImageNet, COCO, Common Crawl) drove innovation.</li> </ul> <p>\ud83d\udc49 In essence, today\u2019s AI giants are standing on the shoulders of selfless community-driven work. Even proprietary models are trained on open datasets, code, and methods published freely.</p>"},{"location":"community-values-guidelines/Prompt-answer/#for-ai-beginners-where-to-focus","title":"\ud83c\udf31 For AI Beginners: Where to Focus","text":"<p>Proprietary tools are great for convenience, but if beginners rely only on them, they risk becoming passive consumers rather than true builders. Here\u2019s a balanced view:</p> <p>1. Start with Community-Driven Resources (Foundations)</p> <ul> <li>Learn basics through open tutorials, GitHub projects, MOOCs, Kaggle notebooks.</li> <li>Experiment with open-source models/datasets \u2014 it teaches how AI really works under the hood.</li> <li>Build small projects (classifiers, chatbots, visualizations) \u2192 gives confidence + transferable skills.</li> </ul> <p>2. Use Proprietary Tools (Practical Exposure)</p> <ul> <li>Once you understand the basics, use ChatGPT, Copilot, or APIs to prototype faster.</li> <li>Learn how enterprises deploy AI \u2192 gives you \u201cindustry language\u201d and real-world workflows.</li> </ul> <p>3. Avoid the Trap of Convenience</p> <ul> <li>Don\u2019t just click buttons. Ask: Could I build a minimal version of this myself with open tools?</li> <li>Treat proprietary platforms as shortcuts, not as your only path.</li> </ul>"},{"location":"community-values-guidelines/Prompt-answer/#rule-of-thumb","title":"\u2696\ufe0f Rule of Thumb","text":"<ul> <li>Beginners \u2192 70% foundation (open, self-built), 30% convenience (proprietary tools).</li> <li>As you advance, you\u2019ll naturally shift \u2014 using open tools for innovation, proprietary tools for scale.</li> </ul> <p>\ud83d\udc49 In short: Learn to cook with raw ingredients (open resources) before ordering takeout every day (proprietary tools).</p>"},{"location":"easy-python-sessions/Sessions/","title":"Easy Learning Sessions","text":""},{"location":"easy-python-sessions/Sessions/#easy-learn-python-for-bigenners-and-non-programmers","title":"Easy Learn Python for Bigenners and Non-Programmers","text":""},{"location":"easy-python-sessions/Sessions/#session-2","title":"Session 2","text":"<ol> <li>Data Types</li> <li>Operators</li> <li>ASCII table</li> <li>Conditions</li> </ol>"},{"location":"easy-python-sessions/Sessions/#session-3","title":"Session 3","text":"<ol> <li>Integers</li> <li>For Loop, positive and reverse</li> <li>Lists</li> <li>Iterate list with For loop</li> </ol>"}]}