{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to docs","text":""},{"location":"#quick-run-mapped-documents-local-site","title":"Quick Run: Mapped documents local site","text":"<pre><code>    python3 -m pip install --upgrade pip setuptools wheel\n    pip install mkdocs==1.5.3\n    pip install mkdocs-material\n    mkdocs serve -a 127.0.0.1:8000\n    mkdocs build\n    mkdocs gh-deploy\n</code></pre>"},{"location":"#step-by-step-guide-to-set-up-mkdocs","title":"Step-by-Step Guide to Set Up MkDocs","text":""},{"location":"#1-initialize-the-git-repository-local","title":"1. Initialize the Git Repository (Local)","text":"<p>First, create a new directory for your project, initialize it as a Git repository, and create a basic README.</p> <pre><code># 1. Create a new directory for your project\nmkdir my-new-docs-repo\n\n# 2. Move into the directory\ncd my-new-docs-repo\n\n# 3. Initialize Git\ngit init\n\n# 4. Create a placeholder README\necho \"# My Project Documentation\" &gt; README.md\n</code></pre>"},{"location":"#2-set-up-the-python-environment-and-dependencies","title":"2. Set up the Python Environment and Dependencies","text":"<p>It's best practice to use a Python virtual environment to isolate your project's dependencies.</p> <pre><code># 1. Create a virtual environment named 'venv'\npython -m venv venv\n\n# 2. Activate the virtual environment\n# On macOS/Linux:\nsource venv/bin/activate\n\n# On Windows (Command Prompt):\n# venv\\Scripts\\activate.bat\n\n# On Windows (PowerShell):\n# venv\\Scripts\\Activate.ps1\n\n# 3. Install MkDocs and the Material theme\npip install mkdocs mkdocs-material\n</code></pre>"},{"location":"#3-initialize-the-mkdocs-project","title":"3. Initialize the MkDocs Project","text":"<p>Now, use the <code>mkdocs</code> command to scaffold the initial file structure.</p> <pre><code># Initialize the MkDocs project\nmkdocs new .\n</code></pre> <p>This command creates two key items:</p> <ol> <li><code>mkdocs.yml</code>: The main configuration file.</li> <li><code>docs/</code> directory: The folder where all your Markdown files will live. It will contain an initial <code>index.md</code> file.</li> </ol>"},{"location":"#4-configure-mkdocsyml","title":"4. Configure <code>mkdocs.yml</code>","text":"<p>Edit the auto-generated <code>mkdocs.yml</code> file to match your desired structure and theme (using the Material theme).</p> <p>Replace the contents of your <code>mkdocs.yml</code> with the following configuration (or adapt it based on your needs):</p> <pre><code>site_name: AI Learning Group\nsite_dir: site\ndocs_dir: docs\nrepo_url: https://github.com/your-username/my-new-docs-repo # &lt;--- UPDATE THIS\nrepo_name: my-new-docs-repo\nedit_uri: edit/main/docs/ # Allows users to click 'Edit this page'\n\n# Configure the Material theme\ntheme:\n  name: material\n  palette:\n    scheme: slate # dark mode\n    primary: deep purple\n    accent: purple\n  features:\n    - navigation.instant\n    - navigation.tabs\n    - search.suggest\n    - toc.follow\n\n# Add necessary extensions and plugins\nmarkdown_extensions:\n  - footnotes\n  - admonition\n  - pymdownx.details\n\nplugins:\n  - search\n\n# Define the navigation structure\nnav:\n  - Home: index.md\n  - Docs:\n    - AI PC Build: ai-pc/pc-for-running-20b-llm.md\n</code></pre> <p>Note on File Structure: For the <code>nav</code> above to work, you must create the directory and file: <code>docs/ai-pc/pc-for-running-20b-llm.md</code>.</p>"},{"location":"#5-create-your-content-files","title":"5. Create Your Content Files","text":"<p>Create the content file referenced in the navigation:</p> <pre><code># Create the subdirectory\nmkdir -p docs/ai-pc\n\n# Create the content file\ntouch docs/ai-pc/pc-for-running-20b-llm.md\n</code></pre> <p>You can now edit <code>docs/index.md</code> and <code>docs/ai-pc/pc-for-running-20b-llm.md</code> with your actual documentation content.</p>"},{"location":"#6-test-locally","title":"6. Test Locally","text":"<p>Run the local development server to see your documentation in action.</p> <pre><code>mkdocs serve -a 127.0.0.1:8000\n</code></pre> <p>Open your browser to the address provided (usually http://127.0.0.1:8000). The server will automatically reload when you make changes to any of your Markdown or configuration files.</p>"},{"location":"#7-commit-and-push-to-git","title":"7. Commit and Push to Git","text":"<p>Once you are happy with the setup, commit your initial files to your local repository and push them to your remote Git hosting service (like GitHub, GitLab, or Bitbucket).</p> <pre><code># 1. Add generated files (excluding the virtual environment)\ngit add .\n\n# 2. (Optional but recommended) Create a .gitignore file\necho \"venv/\" &gt;&gt; .gitignore\ngit add .gitignore\n\n# 3. Commit the initial setup\ngit commit -m \"Initial MkDocs setup with Material theme\"\n\n# 4. Add your remote (if not already done)\n# git remote add origin &lt;your-repo-url&gt;\n\n# 5. Push the changes\n# git push -u origin main\n</code></pre>"},{"location":"Top-Local-LLMs/","title":"Top Local LLMs","text":""},{"location":"Top-Local-LLMs/#top-free-small-language-models-slm-comparison","title":"Top Free Small Language Models (SLM) Comparison","text":"Model Name (Repository) Family/Developer Parameter Size License Key Capabilities (Good At) Validation Gemma 3 - 270M Google / Gemma 270 Million Gemma License Question answering, summarization, and reasoning. Confirmed. Excellent performance for its compact size, especially in common NLP tasks. Qwen3 - 0.6B Alibaba / Qwen 0.6 Billion (600M) Tongyi Qianwen License Switch between \u201cthinking mode\u201d for complex reasoning, math, and coding, and \u201cnon-thinking mode\u201d for fast, general-purpose dialogue. Confirmed. Designed for fast general-purpose dialogue and complex problem-solving in a compact model. SmolLM3 - 3B HuggingFaceTB 3 Billion Apache 2.0 Thinking more for complex problem solving, agentic usage with tool calling, making it versatile for real-world applications. Confirmed. The model is specifically tuned for agentic workflows and complex problem-solving. Qwen3 - 4B-Instruct-2507 Alibaba / Qwen 4 Billion Tongyi Qianwen License Logical reasoning, text comprehension, mathematics, science, coding, and tool usage. Confirmed. This is an instruction-tuned variant optimized for a wide range of academic, reasoning, and coding benchmarks. Gemma 3 - 4B-IT Google / Gemma 4 Billion Gemma License Fine-tuning on text classification, image classification, &amp; specialized tasks. Confirmed. The Instruction-Tuned (<code>-it</code>) variant is optimized for instruction-following and serves as a strong base for further fine-tuning on specialized tasks. Granite 4 (tiny) granite4:tiny-h IBM / Granite 4.0 7 Billion Apache 2.0 Question answering, summarization, classification, code answering, function calling, RAG, and multilingual support. Confirmed. The Granite 4.0-Micro is a 3B model under Apache 2.0, optimized for enterprise tasks like RAG, function calling, and multilingual support."},{"location":"Local_N8N_setup_with_Local_AI/Git-hub-Sersion-Step-by-step-guide-to-setup-n8n-locally-with-llm/","title":"n8n - Step-by-step setup guide with Docker Desktop","text":""},{"location":"Local_N8N_setup_with_Local_AI/Git-hub-Sersion-Step-by-step-guide-to-setup-n8n-locally-with-llm/#demo","title":"Demo","text":""},{"location":"Local_N8N_setup_with_Local_AI/Git-hub-Sersion-Step-by-step-guide-to-setup-n8n-locally-with-llm/#steps","title":"steps","text":"<ol> <li> <p>Download and install docker desktop</p> <p>Note: Its better install with recommended options like wsl</p> <p></p> <p></p> <p>Info: If you installed with default options, the below wsl window will open, you can close it safely.</p> <p></p> </li> <li> <p>After installion is complete, restart might be needed, then lookup \"Docker Desktop\" in windows start meny and open it.</p> <p></p> </li> <li> <p>CLick Search on top and type n8n in seach window, and then click pull for n8n image</p> <p></p> <p></p> <p></p> </li> <li> <p>Run the container and provide the parameters and values shown in screenshot below</p> <p></p> <p></p> <p></p> </li> <li> <p>URL will be generated, open it in browser and create a free account</p> <p></p> <p></p> </li> <li> <p>On home page, click on create workflow to see workflow screen.</p> <p></p> </li> <li> <p>We'll take a step back to add community nodes for n8n</p> <p></p> <p></p> <p></p> </li> <li> <p>Open the new workflow again, search for triggers and add chat trigger, nothing to modify in settings, click back.</p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Now lets look for AI agent and add it, no setting to modify for now, click back and agent should be added with nodes for chat model, memory and tool.</p> <p></p> <p></p> <p></p> </li> <li> <p>Now lets click on chat model node of the agent and search for Ollama chat model , selct it, in config screen, click add credential and enter docker url shown, no api key needed as it local LLM, connection will be tested and click close and select the LLM.NOTE you might need to reopen to see LLM sometimes.</p> <p></p> <p></p> <p></p> </li> <li> <p>Click on memory of the AI agent and add simple memory node.</p> <p> </p> </li> </ol>"},{"location":"Local_N8N_setup_with_Local_AI/Step-by-step-guide-to-setup-n8n-locally-with-llm/","title":"n8n - Step-by-step setup guide with Docker Desktop","text":""},{"location":"Local_N8N_setup_with_Local_AI/Step-by-step-guide-to-setup-n8n-locally-with-llm/#demo","title":"Demo","text":""},{"location":"Local_N8N_setup_with_Local_AI/Step-by-step-guide-to-setup-n8n-locally-with-llm/#steps","title":"steps","text":"<ol> <li> <p>Download and install docker desktop</p> <p>Note: Its better install with recommended options like wsl</p> <p></p> <p></p> <p>Info: If you installed with default options, the below wsl window will open, you can close it safely.</p> <p></p> </li> <li> <p>After installion is complete, restart might be needed, then lookup \"Docker Desktop\" in windows start meny and open it.</p> <p></p> </li> <li> <p>CLick Search on top and type n8n in seach window, and then click pull for n8n image</p> <p></p> <p></p> <p></p> </li> <li> <p>Run the container and provide the parameters and values shown in screenshot below</p> <p></p> <p></p> <p></p> </li> <li> <p>URL will be generated, open it in browser and create a free account</p> <p></p> <p></p> </li> <li> <p>On home page, click on create workflow to see workflow screen.</p> <p></p> </li> <li> <p>We'll take a step back to add community nodes for n8n</p> <p></p> <p></p> <p></p> </li> <li> <p>Open the new workflow again, search for triggers and add chat trigger, nothing to modify in settings, click back.</p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Now lets look for AI agent and add it, no setting to modify for now, click back and agent should be added with nodes for chat model, memory and tool.</p> <p></p> <p></p> <p></p> </li> <li> <p>Now lets click on chat model node of the agent and search for Ollama chat model , selct it, in config screen, click add credential and enter docker url shown, no api key needed as it local LLM, connection will be tested and click close and select the LLM.NOTE you might need to reopen to see LLM sometimes.</p> <p></p> <p></p> <p></p> </li> <li> <p>Click on memory of the AI agent and add simple memory node.</p> <p> </p> </li> </ol>"},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/","title":"Prviate-Secure Docker-Compose n8n + local LLM","text":""},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#n8n-one-step-deploy-on-docker-with-docker-compose","title":"n8n one step deploy on docker with docker-compose","text":""},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#run-with-docker-compose","title":"run with docker compose","text":""},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step-1-open-your-terminal-or-command-prompt","title":"Step 1: Open Your Terminal or Command Prompt","text":"<p>First, you need to open a terminal (on Linux or macOS) or a command prompt/PowerShell (on Windows).</p>"},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step-2-navigate-to-the-files-directory","title":"Step 2: Navigate to the File's Directory","text":"<p>Using the <code>cd</code> (change directory) command, navigate to the folder where you saved your <code>docker-compose.yml</code> file. For example, if you saved the file in a folder called <code>n8n-project</code> on your desktop, you would use a command like this:</p> <p><code>cd ~/Desktop/n8n-project</code></p>"},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#create-a-file-on-your-machine-as-docker-composeyml-below-content","title":"create a file on your machine as 'docker-compose.yml' below content","text":"<pre><code>\nservices:\n  n8n:\n    image: n8nio/n8n:latest\n    container_name: n8n\n    restart: unless-stopped\n    ports:\n      - \"5678:5678\"\n    volumes:\n      - n8n_data:/home/node/.n8n\n    environment:\n      - N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE=true\n      # No need for N8N_LLM_SERVER_URL for Ollama integration &gt;&gt;       http://host.docker.internal:11434\n      - N8N_LLM_SERVER_URL=http://ollama:11434\n    depends_on:\n      - ollama\n    networks:\n      - n8n-ollama-network\n\n\n  ollama:\n    image: ollama/ollama:latest\n    container_name: ollama\n    restart: unless-stopped\n    ports:\n      - \"11434:11434\"  # Expose Ollama on the default port\n    volumes:\n      - ollama_data:/root/.ollama\n\n    networks:\n      - n8n-ollama-network\n\nvolumes:\n  n8n_data:\n  ollama_data:\n\nnetworks:\n  n8n-ollama-network:\n\n</code></pre>"},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step-3-run-the-docker-compose-command","title":"Step 3: Run the Docker Compose Command","text":"<p>Once you are in the correct directory, you can use the following command to start n8n. This command tells Docker Compose to read the <code>docker-compose.yml</code> file and start the services defined within it.</p> <p><code>docker compose up -d</code></p> <ul> <li><code>docker compose up</code>: This part of the command initiates the process of creating and starting the containers.</li> <li><code>-d</code>: This flag stands for \"detached mode.\" It runs the container in the background, so you can continue to use your terminal for other tasks without keeping it open to manage the running container.</li> </ul>"},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step-4-access-n8n","title":"Step 4: Access N8N","text":"<p>After running the command, Docker will download the n8n image (if it's not already on your system) and start the container. You can then access the n8n web interface by opening your web browser and navigating to <code>http://localhost:5678</code>.</p>"},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#local-llms","title":"Local LLMs","text":"<pre><code>$ docker exec -ti ollama /bin/bash\nroot@b65cb311556b:/# ollama pull llama3.2\n</code></pre> <p>Several other GGUF models support tool use and function calling, which are essential for building agents that can interact with external APIs and services. The ability to use tools is a rapidly evolving area in the world of open-source LLMs.</p>"},{"location":"Local_N8N_setup_with_Local_AI/n8n-setup-with-docker-compose/#step5-login-and-create-workflows","title":"Step5: Login and create workflows","text":"<p>Follow the steps from Step 5 onwards in docker-desktop-based-setup</p>"},{"location":"ai-learning-plans/An-approach-to-learn-AI-in-2025-fueled-by-real-world-problem-solving-and-vigor/","title":"2025 - Easy ways to Laern AI","text":""},{"location":"ai-learning-plans/An-approach-to-learn-AI-in-2025-fueled-by-real-world-problem-solving-and-vigor/#objective","title":"Objective","text":"<p>To provide a clear roadmap for beginners to start their AI journey\u2014whether through no-code exploration or coding-based learning\u2014by focusing on practical skills, daily automation, and progressive workflow building.</p>"},{"location":"ai-learning-plans/An-approach-to-learn-AI-in-2025-fueled-by-real-world-problem-solving-and-vigor/#goal","title":"Goal","text":"<ul> <li>Enable beginners to leverage AI effectively in daily life and work.</li> <li>Build confidence in prompting, automation, and workflow design before diving into programming.</li> <li>Offer an alternative technical path (Python + data science libraries) for those aiming at developer or advanced technical roles.</li> <li>Encourage project-based learning and portfolio creation to demonstrate practical expertise.</li> </ul> <ul> <li> <p>Start with No-Code Tools &amp; Accessibility</p> </li> <li> <p>Modern AI tools (e.g., ChatGPT) are built for non-technical users\u2014no coding required.</p> </li> <li>Begin as an \u201cEveryday Explorer\u201d: simplify life, summarize documents, and organize tasks with AI.</li> <li> <p>Even if aiming for technical roles, start with Generative AI (GenAI) first.</p> </li> <li> <p>Prompting is the Core Skill</p> </li> <li> <p>The most essential ability: communicating effectively with AI.</p> </li> <li> <p>Use the simple structure:</p> <ul> <li>Aim \u2192 What you want AI to do</li> <li>Context \u2192 Background/relevant information</li> <li>Rules \u2192 Constraints, format, or style requirements</li> </ul> </li> <li> <p>Practice Through Daily Automation</p> </li> <li> <p>Identify pain points in work/life and apply AI to solve them.</p> </li> <li>Start by automating personal tasks to boost efficiency.</li> <li> <p>Use existing tools (AI Consumers / Level 3), e.g., custom GPTs or content generators.</p> </li> <li> <p>Advance to Workflow Building (No-Code Platforms)</p> </li> <li> <p>Learn \u201cWorkflow Thinking\u201d: break large tasks into smaller steps.</p> </li> <li> <p>Automate and connect steps (\u201cCreative Remixing\u201d) with no-code platforms:</p> <ul> <li>Zapier, Make, N8N, etc.</li> <li>Transition into a Builder (Level 2) \u2192 creating custom tools or agents that can reason and make decisions.</li> </ul> </li> <li> <p>Programming Alternative (If Coding Route Chosen)</p> </li> <li> <p>Set up a stable dev environment (e.g., VS Code).</p> </li> <li> <p>Learn Python fundamentals, then progress to libraries:</p> <ul> <li>NumPy, Pandas, Matplotlib.</li> <li>Focus on project-based learning \u2192 build portfolio projects early.</li> </ul> </li> </ul>"},{"location":"ai-learning-plans/ai-learning-plan-draft/","title":"2025 - AI Learning Path Draft","text":"<p>This roadmap is designed for an absolute beginner with no prior Python knowledge, focusing on the \"Modern Route\" or \"Everyday Explorer/Power User\" approach described in the sources. This strategy emphasizes rapid, impactful learning using accessible tools and prompt engineering, allowing for real-life implementations within a 2-3 month timeframe, without requiring deep theoretical math or coding initially. User are adviced to spend additional time on areas that interests them or skip a step that doesnt interest them. </p> <p>Interactive, Q&amp;A based collaborative learning can help well with rounded progress and widen activities and spin up interesting disucssions for brainstorming and fun all at the same time.</p> <p>If the goal is rapid progress and achieving practical results in 2-3 months, experts suggest starting with Generative AI applications first, rather than the time-consuming traditional route (Math, Stats, foundational ML).</p>"},{"location":"ai-learning-plans/ai-learning-plan-draft/#ai-roadmap-for-absolute-beginners-23-months","title":"AI Roadmap for Absolute Beginners (2\u20133 Months)","text":"<p>The roadmap is divided into three phases, designed to move the learner from being an AI Consumer (Level 3) to an AI Power User/Tool Builder (approaching Level 2), drawing heavily on no-code/low-code tools and fundamental concepts like prompt engineering.</p>"},{"location":"ai-learning-plans/ai-learning-plan-draft/#phase-1-ai-consumer-and-prompt-mastery-weeks-1-4","title":"Phase 1: AI Consumer and Prompt Mastery (Weeks 1-4)","text":"Duration Core Concept &amp; Goal Key Skills &amp; Actionable Steps Implementation/Taste Week 1 Setting the Stage &amp; Identifying Pain Points Initial Survey/Self-Assessment: Identify current knowledge of no-code tools (e.g., ChatGPT, MidJourney, Claude) and document personal \"pain points\" (tasks causing stress or procrastination). Understand that AI is a large umbrella term, and the immediate focus is on Generative AI (GenAI). Start using a foundational LLM (e.g., ChatGPT or Gemini) to explore daily use cases like summarizing content or drafting emails for efficiency. Week 2 Prompting Fundamentals: Aim, Context, Rules Transition from \"playful prompting\" to structured prompting. Learn the core structure of effective prompting: Aim (what the AI should do), Context (background/audience, often using examples), and Rules (limits, formatting, style). Understand key terms like token (for measuring input/output) and hallucination (AI making things up, which requires double-checking). Practice applying the Aim/Context/Rules structure to 5-10 pain points identified in Week 1 to improve output quality dramatically. Week 3 Role Prompting &amp; Research Tools Learn Role Prompting (assigning the AI a persona, e.g., \"You are a business consultant\") to instantly shape the tone and perspective of the response. Explore specialized research tools (e.g., Perplexity or Notebook LM) that use Retrieval Augmented Generation (RAG) to ground answers in real sources, rather than relying only on training data. Use RAG-enabled tools to synthesize information from a large PDF or article, ensuring the answer is grounded in the provided document (practical application of RAG). Week 4 Workflow Thinking &amp; Tool Stacking Master Workflow Thinking\u2014the ability to break a large, complex task into smaller, manageable steps that AI can handle individually. Understand that stacking three to five solid tools is often more powerful than chasing every new release. Combine two different tools (e.g., an LLM for scripting and an image generator for thumbnails) to solve a small, multi-step creative problem (Creative Remixing)."},{"location":"ai-learning-plans/ai-learning-plan-draft/#phase-2-ai-power-user-and-tool-stacking-weeks-5-8","title":"Phase 2: AI Power User and Tool Stacking (Weeks 5-8)","text":"<p>This phase focuses on leveraging specialized tools and combining them, which is where high-impact results often emerge.</p> Duration Core Concept &amp; Goal Key Skills &amp; Actionable Steps Implementation/Taste Week 5 Creative Remixing: Image Generation Explore the Image Category (MidJourney, ChatGPT's generator, Ideogram). Learn about the foundation: Diffusion models (starting with noise and removing it based on the prompt). Understand that many specialized tools online are often just \"specialized wrappers\" built on foundation models. Generate branded graphics, illustrations, or website mock-ups using text-to-image tools. Week 6 Creative Remixing: Audio and Video Explore the Audio Category (e.g., Eleven Labs for Text-to-Speech, Suno/Yo for music generation) and the rapidly moving Video Category (Text-to-Video and Image-to-Video). Produce a short piece of content (e.g., a short video with an AI-generated script, AI-generated voiceover, and AI-generated background music) to see the efficiency of tool stacking. Weeks 7-8 No-Code Automation &amp; Workflow Implementation Learn to automate repetitive tasks. Automations follow a fixed, step-by-step sequence. Use platforms like Naden (popular for workflow templates) or Zapier/Make to connect tools. Understand how to break down a larger goal into sequential steps that can be automated. Achieve a Real-Life Implementation: Set up a simple no-code workflow (e.g., automatically posting content from one platform to another, or summarizing daily calendar events and emailing them)."},{"location":"ai-learning-plans/ai-learning-plan-draft/#phase-3-building-a-foundation-scaling-up-weeks-9-12","title":"Phase 3: Building a Foundation &amp; Scaling Up (Weeks 9-12)","text":"<p>This final phase focuses on introducing more advanced concepts like agents and addressing the barrier of coding/Python, allowing the learner to decide if they want to pursue the full AI Engineer path.</p> Duration Core Concept &amp; Goal Key Skills &amp; Actionable Steps Implementation/Taste Week 9 Agentic AI Fundamentals Understand the difference between fixed Automations and dynamic Agents (which can reason, make decisions, and choose actions). Learn the three necessary components of an agent: a Brain (LLM), Memory (to retain context), and Tools (actions it can take). Experiment with simple agent nodes in automation tools (like Naden) to build a basic AI personal assistant prototype (e.g., reading a calendar and summarizing priorities). Week 10 The Necessity of Python &amp; Data Science Skills Recognize that if the goal shifts from using existing tools to building custom applications or scaling beyond subscription limits, coding becomes necessary. Python is the go-to language for AI and data science. Even if not coding yet, understand the basic necessity of Data Science skills (cleaning, sourcing, pre-processing data) because \"garbage in, garbage out\" applies even to cutting-edge models. If the learner chooses to begin coding, start with Python fundamentals, focusing on data manipulation libraries like NumPy and Pandas, as all AI is created from data. (If they choose the Traditional Route, they would need Math and Statistics next). Week 11 Project Portfolio &amp; Specialization Use the projects and implementations completed in Phases 1 and 2 to build a portfolio. This is the point to pick a specialization (e.g., large language models, computer vision, etc.). Focus on reverse engineering existing projects to learn the structure of code and application building. Work on a project from a resource like Kaggle (for data science) or explore available LangChain experiments (for LLMs) to understand structure, even if just following along. Week 12 Monetization &amp; Continuous Learning Step 7 is to monetize these skills (via a job, freelancing, or building a product). The real learning happens under the pressure of a deadline or client request. Establish a routine for continuous learning and upskilling, perhaps by joining a like-minded community. Based on the 3-month experience, reflect on whether the theoretical foundation (Math, Statistics, traditional ML) is now required to fill technical gaps, or if focusing purely on Generative AI building remains the path."},{"location":"ai-learning-plans/ai-learning-plan-draft/#summary-of-recommended-approach","title":"Summary of Recommended Approach","text":"<p>This fast-track plan aligns with the Modern Route which suggests starting with Generative AI first, mastering application building, and then adding Data Science fundamentals later, as opposed to the Traditional Route which requires mastering Math, Stats, and Classical ML before moving to GenAI.</p> Route Recommended for Beginners (Rapid Progress) Timeframe (Approx.) Modern Route (Recommended) Learn Generative AI/LLMs first (2 months), then gradually add Data Science fundamentals. 2-3 months minimum to get practical experience. Traditional Route Recommended for freshers or those who want a strong foundational base. 8 months total (4 months for DS/ML/CV/NLP + 2 months for GenAI + 2 months for Agentic AI)."},{"location":"ai-pc/hardware-for-running-heavy-llms/","title":"CPU GPU for heavy LLMs","text":""},{"location":"ai-pc/hardware-for-running-heavy-llms/#hardware-configuration-for-medium-to-heavy-llms","title":"Hardware Configuration for Medium to Heavy LLMs","text":"Component For 20B Parameter Models For 80B Parameter Models GPU (NVIDIA) Higher Cost: NVIDIA RTX 4090 (24 GB VRAM)Budget: NVIDIA RTX 3090 (24 GB VRAM) on the used market, or NVIDIA RTX 4080 (16 GB VRAM) for a new card. Higher Cost: NVIDIA RTX 5090 (32 GB VRAM)Budget: Dual NVIDIA RTX 3090s (24 GB VRAM each) or a professional card like the NVIDIA RTX A6000 (48 GB VRAM) if available. GPU (AMD) Best Value: AMD Radeon RX 7900 XTX (24 GB VRAM)Budget: AMD Radeon RX 7900 XT (20 GB VRAM) Higher Cost: AMD Instinct MI300X (192 GB VRAM)Budget: Dual AMD Radeon RX 7900 XTX (24 GB VRAM each) GPU Interconnect N/A High-speed links like NVLink (for NVIDIA) or a motherboard with PCIe 4.0/5.0 for efficient communication between GPUs. CPU (Central Processing Unit) Higher Cost: AMD Ryzen 9 9950X3D or Intel Core i9-14900K.Budget: AMD Ryzen 7 7800X3D or Intel Core i7-14700K. Higher Cost: AMD Ryzen 9 9950X3D or Intel Core i9-14900K. The new AMD Ryzen AI 9 395+ processors also offer excellent performance-per-watt for AI tasks. System RAM Higher Cost: 64 GB+ of DDR5 RAM (e.g., 6000MHz+)Budget: 32 GB of DDR5 RAM (e.g., 5600MHz) Higher Cost: 128 GB+ of DDR5 ECC RAMBudget: 64 GB of DDR5 RAM (e.g., 5600MHz) Storage (NVMe SSD) Higher Cost: A PCIe 5.0 NVMe SSD (e.g., Crucial T705) with sequential read/write speeds of up to 14,000 MB/s.Budget: A high-end PCIe 4.0 NVMe SSD (e.g., Samsung 990 Pro or WD Black SN850X) with speeds of \\~7,000 MB/s. Higher Cost: Multiple high-capacity PCIe 5.0 NVMe SSDs in a RAID configuration for maximum throughput.Budget: A high-capacity PCIe 4.0 NVMe SSD (e.g., Samsung 990 Pro 4TB) with a capacity of 4 TB+. Power Supply A high-wattage PSU (850W+) is recommended. A high-wattage PSU (1000W+) is essential for multi-GPU setups. Cooling Robust cooling for the GPU and CPU is important to prevent performance throttling. Robust cooling for the GPU and CPU is essential to handle the heat from a multi-GPU setup. Interconnect Description Primary Use Case in AI/LLMs Key Advantage Supported GPUs NVLink A high-bandwidth, low-latency GPU-to-GPU interconnect developed by NVIDIA. It allows GPUs to directly share memory and data. Training large models (e.g., LLMs) on multiple GPUs and high-performance computing. Extremely fast direct GPU-to-GPU communication for memory pooling and data sharing. NVIDIA GPUs (specifically data center and high-end workstation models like A100, H100, and some RTX cards) PCIe (PCI Express) A high-speed serial computer expansion bus standard. It is the primary way GPUs connect to a motherboard. Connecting a single GPU to a CPU for both training and inference. Used for multi-GPU setups where the model is too large for one GPU but communication overhead is not a critical bottleneck. Ubiquitous, industry standard, and widely compatible with all consumer and professional GPUs. Both NVIDIA and AMD GPUs Infinity Fabric A cache-coherent interconnect protocol developed by AMD. It provides high-speed communication between different components, including multiple GPUs and CPU dies. Multi-GPU setups for AI workloads on AMD hardware. It allows for efficient data and memory sharing between compatible GPUs. High-speed, low-latency communication specifically designed for AMD's hardware ecosystem. AMD GPUs (Radeon Instinct and some Radeon Pro series) OCuLink An external physical connector for a PCIe channel. It enables a high-speed, direct PCIe connection to an external device, such as an eGPU enclosure. Connecting external GPUs to devices that are not standard desktop PCs (e.g., mini PCs, laptops) for high-performance AI inference or gaming. Provides a pure, low-latency PCIe connection externally, often outperforming alternatives like Thunderbolt for raw bandwidth. Both NVIDIA and AMD GPUs via eGPU docks"},{"location":"ai-pc/pc-for-running-20b-llm/","title":"\ud83d\ude80 Benefits of Running GPT-OSS:20B Locally","text":""},{"location":"ai-pc/pc-for-running-20b-llm/#1-full-data-privacy","title":"\ud83d\udd12 1. Full Data Privacy","text":"<ul> <li>Everything stays on your machine. No queries, prompts, or datasets ever leave your PC.</li> <li>Ideal if you\u2019re working with sensitive business docs, medical/legal notes, financial records, or even personal journals.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#2-latency-responsiveness","title":"\u26a1 2. Latency &amp; Responsiveness","text":"<ul> <li>No internet round-trip.</li> <li>Local inference = instant responses (tokens stream as soon as GPU/CPU generates them).</li> <li>Perfect for offline use, travel, or areas with poor connectivity.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#3-zero-ongoing-api-costs","title":"\ud83d\udcb8 3. Zero Ongoing API Costs","text":"<ul> <li>Once you\u2019ve bought the hardware, inference is free.</li> <li>Running GPT-OSS:20B locally avoids per-token API charges from cloud providers.</li> <li>Useful if you need large batch processing (e.g., analyzing 10,000 PDFs, summarizing datasets).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#4-full-control-customization","title":"\ud83d\udee0\ufe0f 4. Full Control &amp; Customization","text":"<ul> <li>You can fine-tune or LoRA-train models on your own datasets.</li> <li>Ability to swap between different quantizations (4-bit, 8-bit, FP16) to optimize speed vs quality.</li> <li>Run specialized forks of GPT-OSS:20B tuned for coding, reasoning, or dialogue.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#5-bigger-model-better-reasoning","title":"\ud83e\udde0 5. Bigger Model = Better Reasoning","text":"<ul> <li> <p>GPT-OSS:20B (20 billion parameters) is much stronger than 7B or 13B models:</p> </li> <li> <p>More coherent multi-turn conversations.</p> </li> <li>Better code generation &amp; debugging.</li> <li>More accurate reasoning over long documents.</li> <li>Handles longer context windows (if supported by quantization/runtime).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#6-enterprise-research-use","title":"\ud83d\udcc8 6. Enterprise &amp; Research Use","text":"<ul> <li>You can deploy it as an internal assistant in small companies without exposing data to OpenAI/Anthropic/Google.</li> <li>Academics and students can use it for NLP research, AI experiments, or prototyping without cloud restrictions.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#what-can-you-achieve-with-gpt-oss20b-locally","title":"\ud83c\udfaf What Can You Achieve With GPT-OSS:20B Locally?","text":"<p>Here are some practical, concrete goals you could accomplish:</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#productivity-knowledge-work","title":"\ud83d\udd39 Productivity &amp; Knowledge Work","text":"<ul> <li>Summarize large PDFs, books, or technical documents.</li> <li>Draft reports, proposals, or knowledge-base articles.</li> <li>Answer research queries from your local corpus (RAG setup).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#programming-devops","title":"\ud83d\udd39 Programming &amp; DevOps","text":"<ul> <li>Code generation, debugging, and explanations.</li> <li>Local copilots for VS Code, JetBrains, etc.</li> <li>Infrastructure automation with natural language commands (shell + Ansible + Dockerfile generation).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#research-experimentation","title":"\ud83d\udd39 Research &amp; Experimentation","text":"<ul> <li>Fine-tune models on niche data (medical, law, customer service).</li> <li>Evaluate and benchmark new quantization methods.</li> <li>Compare with other OSS models (Llama 2, Mistral, Mixtral).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#private-ai-agents","title":"\ud83d\udd39 Private AI Agents","text":"<ul> <li>Build local chatbots, assistants, or role-playing AIs without external API calls.</li> <li>Integrate with local tools (e.g., calendar, email, files) without data leaving your machine.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#creative-work","title":"\ud83d\udd39 Creative Work","text":"<ul> <li>Generate stories, scripts, and world-building content.</li> <li>Brainstorm new ideas privately (no IP leaks).</li> <li>Assist with language learning or translation.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#pros-of-local-gpt-oss20b-vs-cloud-api","title":"\ud83d\udfe2 Pros of Local GPT-OSS:20B vs Cloud API","text":"<ul> <li>\u2705 Privacy (your data never leaves your machine).</li> <li>\u2705 No API fees.</li> <li>\u2705 Always available, even offline.</li> <li>\u2705 Full customization (quantization, finetuning, RAG integration).</li> <li>\u2705 Model weights are open \u2014 no black-box restrictions.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#cons","title":"\ud83d\udd34 Cons","text":"<ul> <li>\u274c Requires high-end hardware (VRAM + RAM).</li> <li>\u274c Slower than GPT-4-level APIs \u2014 20B \u2248 GPT-3.5 quality, not SOTA.</li> <li>\u274c Energy use (desktop GPUs draw 300\u2013400W).</li> <li>\u274c Setup requires technical comfort (install runtimes, drivers, quantized weights).</li> </ul> <p>\u2728 Bottom line: Running GPT-OSS:20B locally is about independence, privacy, cost savings, and control. It won\u2019t beat GPT-4 or Claude 3.5 in raw quality, but it gives you a serious personal/private AI lab that you can shape to your own needs.</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#budget-option-a-balanced-budget-recommended-for-stability","title":"Budget Option A \u2014 Balanced Budget (recommended for stability)","text":"<p>Estimated total: \u2248 $2,000</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#parts-exacttypical-skus","title":"Parts (exact/typical SKUs)","text":"<ul> <li>CPU: AMD Ryzen 7 7700X \u2014 8c/16t, strong single-thread. \u2014 $320</li> <li>Motherboard: B650 ATX (e.g., MSI B650 Tomahawk or ASUS TUF B650-PLUS) \u2014 AM5, DDR5 support \u2014 $160</li> <li>GPU: AMD Radeon RX 7900 XT (20GB) \u2014 20 GB VRAM (sufficient for 4-bit 20B). Prefer new from ASUS/Sapphire/MSI \u2014 $850\u2013$950</li> <li>RAM: 64 GB (2\u00d732 GB) DDR5-5600/6000 \u2014 DDR5-5600/6000 CL32 \u2014 $180\u2013$260 Note: For strict budget you can choose 32 GB (2\u00d716) now and upgrade later; but 64 GB is safer.</li> <li>Primary NVMe: 1 TB PCIe4 NVMe (Samsung 990 Pro or WD SN850) \u2014 $110\u2013$140</li> <li>PSU: 850 W 80+ Gold (Corsair RM850x / Seasonic Focus GX) \u2014 $100\u2013$150</li> <li>CPU Cooler: 240 mm AIO (NZXT Kraken X53 / Corsair H100) or quality air cooler \u2014 $80\u2013$120</li> <li>Case: Good airflow mid-tower (Fractal Meshify, Phanteks P400A) \u2014 $70\u2013$120</li> <li>Misc (fans, cables, thermal paste): $30\u2013$60</li> </ul> <p>Estimated subtotal: $1,900 (with 32GB RAM) \u2192 $2,050 (with 64GB RAM). Savings vs recommended (~$2,840 parts-only): ~$790\u2013$940 (meets your $800\u2013$1,200 target).</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#why-this-works","title":"Why this works","text":"<ul> <li>RX 7900 XT (20GB) has enough VRAM to hold a 20B model quantized to 4-bit (\u224810\u201312 GB) with room for GPU buffers.</li> <li>Ryzen 7 7700X (8c/16t) is still very capable for token preprocessing and feeding GPU.</li> <li>64 GB RAM ideal; 32 GB will still run 4-bit 20B but leaves less headroom for other apps \u2014 recommended to aim for 64 if budget allows.</li> <li>PCIe4 NVMe ensures quick model load times without needing PCIe5.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#expected-performance-very-approximate","title":"Expected performance (very approximate)","text":"<ul> <li>GPT-OSS 20B (4-bit) on RX 7900 XT \u2192 ~3\u20136 tokens/sec (depends on runtime, quantization, threading).</li> <li>Smaller models (7B/13B) will be much faster (10\u201330 t/s).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#budget-option-b-tighter-value-build-max-savings-used-gpu","title":"Budget Option B \u2014 Tighter Value Build (max savings, used GPU)","text":"<p>Target: aggressive cost reduction while still enabling 4-bit 20B. Use a used GPU (good deal hunting required).</p> <p>Estimated total: \u2248 $1,400 \u2013 $1,600</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#parts-exacttypical-skus_1","title":"Parts (exact/typical SKUs)","text":"<ul> <li>CPU: Ryzen 5 7600X (6c/12t) \u2014 $230</li> <li>Motherboard: B650 or B650M \u2014 $120\u2013$150</li> <li>GPU (used): AMD Radeon RX 6900 XT (16GB) used \u2014 $450\u2013$650 (used market)   Alternative used: RX 7900 XT if you can find one for ~$700\u2013$850 \u2014 better choice if available</li> <li>RAM: 32 GB (2\u00d716) DDR5-5600/6000 \u2014 $120\u2013$160</li> <li>Primary NVMe: 1 TB PCIe4 NVMe \u2014 $100\u2013$120</li> <li>PSU: 850 W 80+ Gold \u2014 $100</li> <li>CPU Cooler: solid air cooler (Noctua NH-U12S) or 240 AIO \u2014 $60\u2013$100</li> <li>Case + misc: $80\u2013$100</li> </ul> <p>Estimated subtotal: $1,400\u2013$1,600 Savings vs recommended (~$2,840): ~$1,200\u2013$1,440</p>"},{"location":"ai-pc/pc-for-running-20b-llm/#tradeoffs-and-notes","title":"Tradeoffs and notes","text":"<ul> <li>RX 6900 XT (16 GB): 16 GB VRAM is enough for 4-bit 20B (~10\u201312GB), but leaves less VRAM room for multi-buffers or memory fragmentation. You must ensure model + runtime buffers fit. Some runtimes may need extra tuning.</li> <li>Ryzen 5 7600X has fewer cores \u2014 will slightly limit preprocessing throughput; CPU may become a mild bottleneck if heavy multitasking.</li> <li>Used GPU risks: warranty, unknown usage, driver quirks. Buy from reputable sellers with returns.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#expected-performance-approx","title":"Expected performance (approx)","text":"<ul> <li>With RX 6900 XT (16GB): ~2\u20134 tokens/sec for 20B (4-bit), depending on runtime and optimizations. Possibly slower than RX 7900 XT.</li> <li>If you find a used RX 7900 XT (~$700\u2013$850), expect similar performance to Balanced Budget (~3\u20136 t/s).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#what-to-upgrade-later-if-you-start-with-tighter-build","title":"What to upgrade later (if you start with tighter build)","text":"<ol> <li>RAM: upgrade from 32 \u2192 64 GB when budget allows (biggest single uplift for stability).</li> <li>GPU: replace used 6900 XT with RX 7900 XT/XTX for faster tokens/sec and more VRAM headroom.</li> <li>NVMe capacity: add a second NVMe for datasets/models to avoid juggling files.</li> </ol>"},{"location":"ai-pc/pc-for-running-20b-llm/#software-runtime-tips-for-both-builds","title":"Software &amp; runtime tips for both builds","text":"<ul> <li>Use llama.cpp with Vulkan backend or a runtime that supports AMD GPU acceleration (Vulkan / ROCm where available).</li> <li>Prefer 4-bit GGUF/GPTQ quantized models for lowest memory footprint.</li> <li>Tune threads (CPU) and <code>n_batch</code>/<code>n_ctx</code> to balance latency vs throughput.</li> <li>Monitor VRAM usage; if model doesn\u2019t fit, try a 4-bit variant with GPTQ/GGUF.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#shopping-tips-cautions","title":"Shopping tips &amp; cautions","text":"<ul> <li>GPU prices fluctuate \u2014 check Newegg, Amazon, local sellers, and reputable used marketplaces (e.g., eBay with buyer protection).</li> <li>If buying used, prefer sellers with return windows and verify photos/serial if possible.</li> <li>RAM timing: buy a tested DDR5 kit (same dual-rank sticks), avoid mixing kits.</li> <li>PSU quality matters \u2014 don\u2019t buy the cheapest. Good PSU protects your expensive GPU/CPU.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#quick-summary-comparison","title":"Quick summary comparison","text":"Build Total est. Savings vs recommended VRAM RAM Perf (20B 4-bit) Balanced Budget (7900 XT + 64GB) $1,900\u2013$2,050 ~$800\u2013$940 20GB 64GB ~3\u20136 t/s Tighter Value (used 6900 XT + 32GB) $1,400\u2013$1,600 ~$1,200\u2013$1,440 16GB (used) 32GB ~2\u20134 t/s"},{"location":"ai-pc/pc-for-running-20b-llm/#final-recommendation","title":"Final recommendation","text":"<ul> <li>If you want reliable interactive use and longevity, go with Balanced Budget (Option A) \u2014 it meets your $800\u2013$1,200 savings target and keeps good headroom.</li> <li>If your goal is maximum savings now and you\u2019re comfortable risk-managing used parts and future upgrades, use Option B and plan to upgrade RAM/GPU later.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#why-ryzen-9-7950x-rx-7900-xtx-64-gb-updated-memory-pcie-ssd","title":"Why Ryzen 9 7950X + RX 7900 XTX + 64 GB (updated: memory, PCIe, SSD)","text":""},{"location":"ai-pc/pc-for-running-20b-llm/#quick-summary-of-the-new-details","title":"Quick summary of the new details","text":"<ul> <li>RAM type: DDR5 is recommended for Ryzen 9 7950X (speed and Infinity Fabric behavior).</li> <li>RAM speed: Aim for DDR5-6000 MT/s (or 5600\u20136400 MT/s) CL30\u201336 for best latency/throughput balance on Zen4.</li> <li>PCIe: PCIe 4.0 x16 is sufficient for a GPU like RX 7900 XTX; PCIe 5.0 is beneficial for NVMe SSDs and future GPUs but not required for GPU performance today.</li> <li>NVMe SSD: Use PCIe 4.0 NVMe with sustained sequential reads ~5\u20137 GB/s (5000\u20137000 MB/s). If you want the fastest possible model load times and you have a PCIe5 motherboard, PCIe 5.0 NVMe drives (read ~10\u201312+ GB/s) are an extra boost for model I/O.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#why-these-choices-matter-technical-view","title":"Why these choices matter (technical view)","text":""},{"location":"ai-pc/pc-for-running-20b-llm/#ddr5-vs-ddr4-and-speed","title":"DDR5 vs DDR4 (and speed)","text":"<ul> <li> <p>Ryzen 9 7950X (Zen4):</p> </li> <li> <p>Designed for DDR5. DDR5 gives higher bandwidth which helps when the CPU is moving large model chunks and when the system uses RAM as staging for GPU transfers.</p> </li> <li>Recommended speed: DDR5-6000 MT/s is a well-known sweet spot for Ryzen 7000-series \u2014 it often allows Infinity Fabric to run 1:1 (better latency) and yields the best practical throughput. DDR5-5600 \u2192 good, DDR5-6400 \u2192 sometimes faster but may need manual tuning/timings.</li> <li>Timings: lower CAS latency helps, but MT/s matters more for AI workloads (bandwidth-bound). Typical good kit: 6000 MT/s CL30\u201336.</li> <li> <p>If you choose Ryzen 5000 (5900X):</p> </li> <li> <p>DDR4-3600 CL16 is ideal. DDR5 is not supported by that platform.</p> </li> <li> <p>Why not DDR4 on 7950X?</p> </li> <li> <p>7950X motherboards support DDR5 only; DDR4 is incompatible. For other CPUs, DDR4 is fine but lower bandwidth.</p> </li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#capacity-64-gb-vs-32-gb","title":"Capacity (64 GB vs 32 GB)","text":"<ul> <li> <p>64 GB gives headroom for:</p> </li> <li> <p>FP16/8-bit weights and runtime buffers</p> </li> <li>caching multiple models, datasets, or running containers/IDE alongside inference</li> <li>avoiding swap (swap kills throughput)</li> <li>32 GB may be okay for 4-bit 20B but is tight and risky.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#pcie-40-vs-pcie-50","title":"PCIe 4.0 vs PCIe 5.0","text":"<ul> <li>GPU: RX 7900 XTX runs perfectly on PCIe 4.0 x16. Moving to PCIe 5.0 gives little-to-no real-world inference speedup today because the GPU computation is the bottleneck, not PCIe link bandwidth for steady-state inference.</li> <li>NVMe SSDs: Where PCIe5 shines \u2014 PCIe 5.0 NVMe offers ~10\u201314 GB/s sequential reads and speeds up model load times and swap-backed workloads significantly. If you often load many model checkpoints or large datasets, PCIe5 NVMe helps.</li> <li>Practical: Buy a good PCIe4 NVMe unless you specifically want the fastest possible model load times and have a PCIe5 board.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#nvme-ssd-readwrite-what-to-pick","title":"NVMe SSD Read/Write (what to pick)","text":"<ul> <li> <p>Good target for model work:</p> </li> <li> <p>PCIe4 NVMe: Sequential read 5,000\u20137,000 MB/s; write similar for high-end drives. Examples: Samsung 990 Pro, WD Black SN850 \u2013 fast and reliable.</p> </li> <li>PCIe5 NVMe: Sequential read 10,000\u201314,000 MB/s; cutting-edge (requires PCIe5 motherboard).</li> <li>Why sequential read matters: Model files are large sequential reads when loading weights; higher read speeds reduce model startup time and any disk-backed paging overhead.</li> <li>IOPS &amp; endurance: For cache-heavy usage, consider high IOPS and higher TBW (endurance) ratings if you will swap or write a lot.</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#updated-comparison-table-includes-ddr-pcie-ssd-speeds","title":"Updated comparison table (includes DDR, PCIe, SSD speeds)","text":"Build CPU GPU RAM (type &amp; speed) RAM capacity PCIe NVMe suggestion (read/write) Notes &amp; expected t/s Recommended Ryzen 9 7950X (16c/32t) RX 7900 XTX (24 GB) DDR5-6000 MT/s (CL30-36) 64 GB PCIe 4.0 x16 (PCIe5 ready mobo optional) PCIe4 NVMe: 5\u20137 GB/s (PCIe5: 10\u201314 GB/s) Best balance; smooth FP16/INT8 20B inference (~6\u201312 t/s) High-end pro Threadripper 3965WX Radeon PRO W6800 (32GB) DDR4/DDR5 ECC (platform dependent) 128 GB+ PCIe4/PCIe5 server board NVMe PCIe4/5: 7\u201312+ GB/s Multi-model / heavy fine-tuning (10\u201320 t/s) Mid / cost-aware Ryzen 9 5900X (12c/24t) RX 7900 XT/XTX DDR4-3600 (if 5900X) or DDR5 on other CPUs 32\u201364 GB (64 preferred) PCIe 4.0 PCIe4 NVMe: 5\u20137 GB/s Good value; upgrade RAM to 64GB for FP16 20B (~4\u20139 t/s) Budget Ryzen 7 7700X / 5800X3D RX 7900 XT or used 6900 XT DDR5-5200\u20135600 (for 7700X) or DDR4-3600 32 GB PCIe4 PCIe4 NVMe: ~5 GB/s Can run quantized 20B (4-bit), slower (~3\u20136 t/s) Laptop / small Ryzen 9 8945HS Radeon 780M (iGPU) DDR5 LPDDR5x (integrated) 32 GB \u2014 NVMe PCIe4: 5 GB/s Can run 4-bit 20B but mostly CPU-bound (&lt;1\u20132 t/s)"},{"location":"ai-pc/pc-for-running-20b-llm/#concrete-component-recommendations","title":"Concrete component recommendations","text":"<ul> <li> <p>RAM (Ryzen 7950X combo):</p> </li> <li> <p>Buy 2\u00d732 GB DDR5-6000 CL30 (dual-channel) or 4\u00d716 GB if motherboard supports quad. Good kits: reputable brands (G.Skill Trident Z5, Corsair Dominator DDR5).</p> </li> <li>On Ryzen 7000, DDR5-6000 often hits a good Infinity Fabric 1:1 ratio.</li> <li> <p>SSD:</p> </li> <li> <p>Primary NVMe (OS + models): 1\u20132 TB PCIe 4.0 NVMe (Samsung 990 Pro, WD SN850) \u2014 read ~7000 MB/s.</p> </li> <li>Optional ultra-fast scratch: If you have PCIe5 board, consider a PCIe5 drive for super-fast model loads.</li> <li> <p>Motherboard:</p> </li> <li> <p>X670E/B650E for Ryzen 7000 \u2013 choose one with multiple M.2 NVMe slots and PCIe 4.0/5.0 support if future-proofing.</p> </li> <li> <p>PSU &amp; Cooling:</p> </li> <li> <p>PSU: 850\u20131000 W high-quality Gold/Plat</p> </li> <li>Cooling: 240\u2013360 mm AIO for 7950X sustained loads</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#practical-tuning-tips","title":"Practical tuning tips","text":"<ul> <li>Memory config: Use dual-channel kits (2 sticks) for simplicity; 64 GB as 2\u00d732 GB is ideal. Populate recommended slots per manual.</li> <li>RAM speed tuning: Set XMP/EXPO to kit speed (6000) and verify FCLK (Infinity Fabric) settings \u2014 many motherboards support 1:1 at 6000.</li> <li>SSD configuration: Put models on the fastest NVMe drive. If using swap, use a fast NVMe with high TBW.</li> <li>PCIe lanes: Make sure the GPU gets full x16 lanes (most desktop boards give this to GPU slot).</li> </ul>"},{"location":"ai-pc/pc-for-running-20b-llm/#short-checklist-when-buyingbuilding-for-gpt-oss-20b","title":"Short checklist when buying/building for GPT-OSS 20B","text":"<ol> <li>CPU: Ryzen 9 7950X or equivalent desktop-class CPU (&gt;=12 cores recommended).</li> <li>GPU: Discrete GPU with \u226524 GB VRAM (RX 7900 XTX or equivalent).</li> <li>RAM: 64 GB DDR5 (DDR5-6000 kit recommended).</li> <li>Storage: PCIe4 NVMe (1\u20132 TB) \u2014 PCIe5 optional for fastest loads.</li> <li>Motherboard: X670E/B650E with multiple M.2 slots.</li> <li>PSU/Cooling: 850\u20131000W PSU, quality AIO cooler.</li> </ol>"},{"location":"easy-python-sessions/Sessions/","title":"Easy Learning Sessions","text":""},{"location":"easy-python-sessions/Sessions/#session-links","title":"Session Links","text":""},{"location":"easy-python-sessions/Sessions/#session-3","title":"Session 3","text":"<ol> <li>Integers</li> <li>For Loop, positive and reverse</li> <li>Lists</li> <li>Iterate list with For loop</li> </ol>"}]}